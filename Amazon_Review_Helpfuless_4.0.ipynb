{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Sklearn libraries.\n",
    "from sklearn import datasets, linear_model, ensemble, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "#deep learning library\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, tf_embed_viz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip gz file.\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "# Load JSON into dataframe.\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Used 42.877285s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = getDF('/home/legu/data/reviews_Clothing_Shoes_and_Jewelry_5.json.gz')\n",
    "end = time.time()\n",
    "print \"Time Used %fs\" %(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'reviewerID', u'asin', u'reviewerName', u'helpful', u'unixReviewTime',\n",
      "       u'reviewText', u'overall', u'reviewTime', u'summary'],\n",
      "      dtype='object')\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1KLRMWW2FWPL4</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer \"cameramom\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1297468800</td>\n",
       "      <td>This is a great tutu and at a really great pri...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>02 12, 2011</td>\n",
       "      <td>Great tutu-  not cheaply made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2G5TCU2WDFZ65</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1358553600</td>\n",
       "      <td>I bought this for my 4 yr old daughter for dan...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 19, 2013</td>\n",
       "      <td>Very Cute!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1RLQXYNCMWRWN</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Carola</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1357257600</td>\n",
       "      <td>What can I say... my daughters have it in oran...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 4, 2013</td>\n",
       "      <td>I have buy more than one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A8U3FAMSJVHS5</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Caromcg</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1398556800</td>\n",
       "      <td>We bought several tutus at once, and they are ...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>04 27, 2014</td>\n",
       "      <td>Adorable, Sturdy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3GEOILWLK86XM</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>CJ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1394841600</td>\n",
       "      <td>Thank you Halo Heaven great product for Little...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>Grammy's Angels Love it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                 reviewerName helpful  \\\n",
       "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
       "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
       "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
       "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
       "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
       "\n",
       "   unixReviewTime                                         reviewText  overall  \\\n",
       "0      1297468800  This is a great tutu and at a really great pri...   5.0000   \n",
       "1      1358553600  I bought this for my 4 yr old daughter for dan...   5.0000   \n",
       "2      1357257600  What can I say... my daughters have it in oran...   5.0000   \n",
       "3      1398556800  We bought several tutus at once, and they are ...   5.0000   \n",
       "4      1394841600  Thank you Halo Heaven great product for Little...   5.0000   \n",
       "\n",
       "    reviewTime                        summary  \n",
       "0  02 12, 2011  Great tutu-  not cheaply made  \n",
       "1  01 19, 2013                    Very Cute!!  \n",
       "2   01 4, 2013       I have buy more than one  \n",
       "3  04 27, 2014               Adorable, Sturdy  \n",
       "4  03 15, 2014        Grammy's Angels Love it  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.columns\n",
    "\n",
    "print df['reviewText'][0]\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring helpfulness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helpfulness = []\n",
    "total_votes = []\n",
    "for i in df['helpful']:\n",
    "    if i[1] == 0:\n",
    "        helpfulness.append(np.nan)\n",
    "        total_votes.append(np.nan)\n",
    "    else:\n",
    "        helpfulness.append(float(i[0])/i[1])\n",
    "        total_votes.append(i[1])\n",
    "        \n",
    "# Convert to numpy array.\n",
    "helpfulness = np.array(helpfulness)\n",
    "total_votes = np.array(total_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot boxplot.\n",
    "nonnan_helpfulness = helpfulness[~np.isnan(helpfulness)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove invalid and nan values.\n",
    "helpfulness_clean = np.delete(helpfulness, 30730)\n",
    "\n",
    "nonnan_helpfulness_clean = helpfulness_clean[~np.isnan(helpfulness_clean)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract the review text\n",
    "doc = np.array(df['reviewText'])\n",
    "\n",
    "# Filter down to reviews with helpfulness scores.\n",
    "# Remove item with invalid helpfulness score.\n",
    "doc_clean = np.delete(doc, 30730)\n",
    "nonnan_doc_clean = doc_clean[~np.isnan(helpfulness_clean)]\n",
    "y = np.reshape(nonnan_helpfulness_clean,(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep Learning Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Split the dataset into traing and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(nonnan_doc_clean )\n",
    "msk = np.random.rand(nonnan_doc_clean.shape[0]) <= 0.7\n",
    "\n",
    "X_train = nonnan_doc_clean[msk]\n",
    "X_test = nonnan_doc_clean[~msk]\n",
    "y_train = y[msk]\n",
    "y_test = y[~msk]\n",
    "X_sample = nonnan_doc_clean[0:60000]\n",
    "y_sample = y[0:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For what I paid for two tutus is unbeatable anywhere!  I ordered a pink and turquios and they are vibrant and beautiful! The tutu is very full! Princess style! Not cheaply made! Not cheap materia! Obviously someone made these with love and care! I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two ! Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60878\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print (len(X_train))\n",
    "print (len(y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Convert the word into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(docs):\n",
    "    tokens =[]\n",
    "    for i in docs:\n",
    "        lowers = i.lower()\n",
    "        #remove the punctuation using the character deletion step of translate\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token = nltk.word_tokenize(no_punctuation)\n",
    "        tokens.extend(token)\n",
    "    return tokens\n",
    "\n",
    "tokens = get_tokens(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic, rev_dict = build_dataset(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['limited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'limited'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_dict[2032]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sample_ids = []\n",
    "for i in X_sample:\n",
    "    lowers = i.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token = nltk.word_tokenize(no_punctuation)\n",
    "    j=[dic[w] for w in token]\n",
    "    X_sample_ids.append(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For what I paid for two tutus is unbeatable anywhere!  I ordered a pink and turquios and they are vibrant and beautiful! The tutu is very full! Princess style! Not cheaply made! Not cheap materia! Obviously someone made these with love and care! I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two ! Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!\n",
      "[8, 86, 1, 611, 8, 134, 13856, 6, 5996, 1122, 1, 84, 3, 567, 2, 36860, 2, 12, 13, 1811, 2, 255, 0, 6132, 6, 26, 440, 3268, 194, 19, 1510, 90, 19, 259, 80013, 1183, 479, 90, 17, 20, 55, 2, 623, 1, 611, 326, 52, 415, 1225, 8, 3, 6132, 1, 2, 1, 115, 3151, 7, 11, 1969, 8, 5697, 4, 0, 580, 7, 813, 78856, 3345, 39652, 414, 6, 134, 460, 29, 6679, 2, 10, 615, 37, 29, 32, 96, 2, 42, 76260, 28, 338, 4, 1768, 206, 473, 2884, 225, 8, 87, 76]\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "print (X_sample[0])\n",
    "print (X_sample_ids[0])\n",
    "print (y_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.build tensor flow model - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine the list of words index and label into a dataframe\n",
    "train_df = pd.DataFrame(np.column_stack([X_sample_ids,y_sample]), \n",
    "                               columns=['list_words', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 86, 1, 611, 8, 134, 13856, 6, 5996, 1122, ...</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48...</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[55, 10, 494, 270, 22, 46, 141, 661, 610, 8284...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          list_words  label\n",
       "0  [8, 86, 1, 611, 8, 134, 13856, 6, 5996, 1122, ... 0.8750\n",
       "1  [440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9... 1.0000\n",
       "2  [3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48... 0.5000\n",
       "3  [0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ... 1.0000\n",
       "4  [55, 10, 494, 270, 22, 46, 141, 661, 610, 8284... 0.0000"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##this is not necessary\n",
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "        return res['list_words'], res['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [1, 24, 938, 8, 3, 177, 8643, 37, 8, 109, 39, ...\n",
      "1    [17, 13, 3, 38, 77, 7, 379, 34, 23, 27, 10, 46...\n",
      "2    [1, 84, 3, 29, 83, 34, 23, 18, 3, 1113, 643, 2...\n",
      "Name: list_words, dtype: object\n",
      "0   0.5000\n",
      "1   1.0000\n",
      "2   0.7500\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(train_df)\n",
    "d = data.next_batch(3)\n",
    "print(d[0])\n",
    "print(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##this helps to make sure each sentence have the same length (max_len)\n",
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n, max_len):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "#             self.max_len = max_len\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        maxlen = max_len\n",
    "        x = np.zeros([n, maxlen], dtype=np.int32)\n",
    "        for i, x_i in enumerate(x):\n",
    "            l=len(res['list_words'].values[i]) ##list length\n",
    "            if l>maxlen: \n",
    "                x_i[:maxlen] = res['list_words'].values[i][:max_len]\n",
    "            else:\n",
    "                x_i[:l] = res['list_words'].values[i][:l]\n",
    "\n",
    "        return x, res['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1    18   709   198  7592   940   681     0   377 13225    13  2226\n",
      "     20    87   130   631   403    25  2466  1154     9   190   308    41\n",
      "      7   190    74  7592   681  1041    17    18   209   237     1    84\n",
      "      3    29   231    52     0   377   266     2    17    32   233    12\n",
      "    416  1429    46    12   209    73     0   168    25     0  2142 13225\n",
      "      9    29     2    13     3   101   231    52     0   597 61076    18\n",
      "     71   809   136   102    17    42   193    15    56   204     8   116\n",
      "     12    13    50    26   175     1   279    64   172     1    18    21\n",
      "     16     2    12   894    80     0  4350  1176   505     8    48    23\n",
      "     30  1129   153    31   993    14 44314  1829   872    12    13    19\n",
      "    293    44   219    22    14     1    54   100    30   153    14    13\n",
      "    293   394   798     0  1089   225    15   219   118    14    98     6\n",
      "     71  1176   505    20     3   734]\n",
      " [   10     6    19   218     4    29     0   493   182     6   279   134\n",
      "    540   237    52    86     5   227   524     1  1583     4 10436    26\n",
      "    392     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [   95    17     8    11   973  2725    14     5   421  1424     2    14\n",
      "     24     3    57  2507  1411    16    11   364    12    67    44   104\n",
      "      9     0   733     2    44    83     9     0   225     2    41   106\n",
      "      0    32    24    33   111   132     0    76     1   611   282  3350\n",
      "      2    12  3793    11  1275     1    95   825    91    16     3  1265\n",
      "     77     7   412    42   197    93   559   189    14   360  1424    20\n",
      "    609  1275     1   265    25    46    18  1073   964     2  5729     5\n",
      "     16  2817    46    14   439    18  4213     0   107     7  8201 21341\n",
      "      2   300     4     0  1284  1026     1   127  4372   469     8   346\n",
      "  10035    44   566     2   262    27     4  3542   730    14   277   559\n",
      "     16   161   169     0   282  3892     6    70  1344     9 11297 19375\n",
      "      1   835   319  3210     0  1411     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]\n",
      " [    1    95    17     8    11 62897    26  1677    15   157   380  4941\n",
      "     32   510    57    47    78    12   273   218     4    29     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0]]\n",
      "0   1.0000\n",
      "1   1.0000\n",
      "2   0.6667\n",
      "3   1.0000\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_pad = PaddedDataIterator(train_df)\n",
    "d = data_pad.next_batch(4,150)\n",
    "print(d[0]) \n",
    "print(d[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(4,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1341,    10,   387,     6,    26,   673,     1,    65,  3071,\n",
       "            2,     3,    29,  6988,  1590,     1,    65,   448,    29,\n",
       "         1559,     2,  1071,     9,   195,     1,   510,     5,     0,\n",
       "           76,   221,     6,     0,  1590,   494,     6,   121,     4,\n",
       "          120,     8,    45,    72,  2201,     5,   232,    45,   117,\n",
       "           38,     1,    67,    19,    31,     5,    71,     5,   188,\n",
       "           36,     0,  1320,    24,    34,    21,    13,   546,     2,\n",
       "           18,  1901, 20903,    10,    41,    69,    38,    16,    21,\n",
       "           72,  2201,     5,   315,    36,   124,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [   25,     3,    77,   440,    20,    10,   432,   106,     7,\n",
       "           39,     5,     6,   156, 30112,     1,    25,   125,     8,\n",
       "            3,   329,   432,   369,     1,   204,     7,    63,     3,\n",
       "           29,  9368,    63,    68,   776,    79,   153,  1069,   953,\n",
       "           10,   432,   331,   156,    43,    48,    47,     1,   280,\n",
       "            5,    39,     1,   140,   137,     6, 18241,   224,  1609,\n",
       "        11945,   104,   432,     2,   279,    64,     5,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [   40,     3,   107,   518,     8,     3,    90,    30,  1420,\n",
       "         1689,   789,   297,    15,  1341,   210,   560,   347, 26516,\n",
       "            7, 27160,   648,     7,   305,     9,   371,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    1,    65,    24,   220,    20,    17,    12,   117,    38,\n",
       "            2,    26,   419,     0,   313,   838,    25,   567,     1,\n",
       "           65, 16179,     2,     1,    96,  5163,    80,    36,   230,\n",
       "            0,   985,   201,     6,    26,    50,   606,     4,    72,\n",
       "          566,     0,   213,     6,    26,   661,    19,    49,   943,\n",
       "           15,    19, 25981,   275,   397,     5,    79,     6,    19,\n",
       "         4991,    28,    72,  3869,  1374,    67,     1,   874,   111,\n",
       "         4062,    12,    13,    24,    50,     2,     1,   505,     4,\n",
       "           58,   160,    44,   424,    54,     7,    17,     1,    65,\n",
       "           24,   922,    20,    17,     1,    37,    63,   207,    44,\n",
       "          270,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]], dtype=int32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:100: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Minibatch Loss= 1.706890, Training Accuracy= 0.17300\n",
      "Iter 2000, Minibatch Loss= 1.633639, Training Accuracy= 0.19600\n",
      "Iter 3000, Minibatch Loss= 1.716699, Training Accuracy= 0.14100\n",
      "Iter 4000, Minibatch Loss= 1.588266, Training Accuracy= 0.18100\n",
      "Iter 5000, Minibatch Loss= 1.569012, Training Accuracy= 0.15800\n",
      "Iter 6000, Minibatch Loss= 1.512459, Training Accuracy= 0.17400\n",
      "Iter 7000, Minibatch Loss= 1.514805, Training Accuracy= 0.15500\n",
      "Iter 8000, Minibatch Loss= 1.473343, Training Accuracy= 0.16500\n",
      "Iter 9000, Minibatch Loss= 1.419694, Training Accuracy= 0.17800\n",
      "Iter 10000, Minibatch Loss= 1.397555, Training Accuracy= 0.18800\n",
      "Iter 11000, Minibatch Loss= 1.420717, Training Accuracy= 0.16900\n",
      "Iter 12000, Minibatch Loss= 1.358751, Training Accuracy= 0.17800\n",
      "Iter 13000, Minibatch Loss= 1.323090, Training Accuracy= 0.19600\n",
      "Iter 14000, Minibatch Loss= 1.290656, Training Accuracy= 0.18300\n",
      "Iter 15000, Minibatch Loss= 1.268055, Training Accuracy= 0.20000\n",
      "Iter 16000, Minibatch Loss= 1.254888, Training Accuracy= 0.19600\n",
      "Iter 17000, Minibatch Loss= 1.170570, Training Accuracy= 0.22800\n",
      "Iter 18000, Minibatch Loss= 1.177995, Training Accuracy= 0.21100\n",
      "Iter 19000, Minibatch Loss= 1.169791, Training Accuracy= 0.21200\n",
      "Iter 20000, Minibatch Loss= 1.115448, Training Accuracy= 0.24200\n",
      "Iter 21000, Minibatch Loss= 1.074702, Training Accuracy= 0.25400\n",
      "Iter 22000, Minibatch Loss= 1.083880, Training Accuracy= 0.22500\n",
      "Iter 23000, Minibatch Loss= 1.072046, Training Accuracy= 0.27200\n",
      "Iter 24000, Minibatch Loss= 1.000452, Training Accuracy= 0.32200\n",
      "Iter 25000, Minibatch Loss= 0.967615, Training Accuracy= 0.34500\n",
      "Iter 26000, Minibatch Loss= 0.972766, Training Accuracy= 0.35800\n",
      "Iter 27000, Minibatch Loss= 0.946002, Training Accuracy= 0.37300\n",
      "Iter 28000, Minibatch Loss= 0.929282, Training Accuracy= 0.41200\n",
      "Iter 29000, Minibatch Loss= 0.927044, Training Accuracy= 0.44200\n",
      "Iter 30000, Minibatch Loss= 0.874036, Training Accuracy= 0.51700\n",
      "Iter 31000, Minibatch Loss= 0.861909, Training Accuracy= 0.51100\n",
      "Iter 32000, Minibatch Loss= 0.859524, Training Accuracy= 0.55200\n",
      "Iter 33000, Minibatch Loss= 0.809602, Training Accuracy= 0.56200\n",
      "Iter 34000, Minibatch Loss= 0.808587, Training Accuracy= 0.60200\n",
      "Iter 35000, Minibatch Loss= 0.774773, Training Accuracy= 0.60400\n",
      "Iter 36000, Minibatch Loss= 0.759528, Training Accuracy= 0.63200\n",
      "Iter 37000, Minibatch Loss= 0.726397, Training Accuracy= 0.68600\n",
      "Iter 38000, Minibatch Loss= 0.757665, Training Accuracy= 0.66400\n",
      "Iter 39000, Minibatch Loss= 0.719324, Training Accuracy= 0.70200\n",
      "Iter 40000, Minibatch Loss= 0.758828, Training Accuracy= 0.68400\n",
      "Iter 41000, Minibatch Loss= 0.747012, Training Accuracy= 0.68800\n",
      "Iter 42000, Minibatch Loss= 0.733207, Training Accuracy= 0.69000\n",
      "Iter 43000, Minibatch Loss= 0.781603, Training Accuracy= 0.69100\n",
      "Iter 44000, Minibatch Loss= 0.720722, Training Accuracy= 0.71600\n",
      "Iter 45000, Minibatch Loss= 0.736049, Training Accuracy= 0.72400\n",
      "Iter 46000, Minibatch Loss= 0.704529, Training Accuracy= 0.73000\n",
      "Iter 47000, Minibatch Loss= 0.690214, Training Accuracy= 0.73800\n",
      "Iter 48000, Minibatch Loss= 0.693732, Training Accuracy= 0.72700\n",
      "Iter 49000, Minibatch Loss= 0.723175, Training Accuracy= 0.73000\n",
      "Iter 50000, Minibatch Loss= 0.681327, Training Accuracy= 0.75500\n",
      "Iter 51000, Minibatch Loss= 0.685207, Training Accuracy= 0.75900\n",
      "Iter 52000, Minibatch Loss= 0.758390, Training Accuracy= 0.71200\n",
      "Iter 53000, Minibatch Loss= 0.719625, Training Accuracy= 0.73500\n",
      "Iter 54000, Minibatch Loss= 0.708107, Training Accuracy= 0.73400\n",
      "Iter 55000, Minibatch Loss= 0.706041, Training Accuracy= 0.73100\n",
      "Iter 56000, Minibatch Loss= 0.655061, Training Accuracy= 0.78300\n",
      "Iter 57000, Minibatch Loss= 0.672975, Training Accuracy= 0.75100\n",
      "Iter 58000, Minibatch Loss= 0.669277, Training Accuracy= 0.75500\n",
      "Iter 59000, Minibatch Loss= 0.677567, Training Accuracy= 0.73200\n",
      "Iter 60000, Minibatch Loss= 0.658187, Training Accuracy= 0.75500\n",
      "Iter 61000, Minibatch Loss= 0.645865, Training Accuracy= 0.75500\n",
      "Iter 62000, Minibatch Loss= 0.658007, Training Accuracy= 0.76700\n",
      "Iter 63000, Minibatch Loss= 0.698073, Training Accuracy= 0.73000\n",
      "Iter 64000, Minibatch Loss= 0.693780, Training Accuracy= 0.74500\n",
      "Iter 65000, Minibatch Loss= 0.655148, Training Accuracy= 0.76700\n",
      "Iter 66000, Minibatch Loss= 0.665889, Training Accuracy= 0.73600\n",
      "Iter 67000, Minibatch Loss= 0.656864, Training Accuracy= 0.73900\n",
      "Iter 68000, Minibatch Loss= 0.669316, Training Accuracy= 0.75000\n",
      "Iter 69000, Minibatch Loss= 0.647731, Training Accuracy= 0.77200\n",
      "Iter 70000, Minibatch Loss= 0.646717, Training Accuracy= 0.76600\n",
      "Iter 71000, Minibatch Loss= 0.641017, Training Accuracy= 0.77600\n",
      "Iter 72000, Minibatch Loss= 0.610386, Training Accuracy= 0.77600\n",
      "Iter 73000, Minibatch Loss= 0.631920, Training Accuracy= 0.76600\n",
      "Iter 74000, Minibatch Loss= 0.625199, Training Accuracy= 0.78900\n",
      "Iter 75000, Minibatch Loss= 0.614088, Training Accuracy= 0.79100\n",
      "Iter 76000, Minibatch Loss= 0.641385, Training Accuracy= 0.77100\n",
      "Iter 77000, Minibatch Loss= 0.626591, Training Accuracy= 0.77200\n",
      "Iter 78000, Minibatch Loss= 0.667046, Training Accuracy= 0.75800\n",
      "Iter 79000, Minibatch Loss= 0.637928, Training Accuracy= 0.76600\n",
      "Iter 80000, Minibatch Loss= 0.641758, Training Accuracy= 0.77400\n",
      "Iter 81000, Minibatch Loss= 0.649422, Training Accuracy= 0.76700\n",
      "Iter 82000, Minibatch Loss= 0.618821, Training Accuracy= 0.78700\n",
      "Iter 83000, Minibatch Loss= 0.629348, Training Accuracy= 0.79400\n",
      "Iter 84000, Minibatch Loss= 0.601197, Training Accuracy= 0.80000\n",
      "Iter 85000, Minibatch Loss= 0.646581, Training Accuracy= 0.76300\n",
      "Iter 86000, Minibatch Loss= 0.620228, Training Accuracy= 0.78300\n",
      "Iter 87000, Minibatch Loss= 0.612948, Training Accuracy= 0.79000\n",
      "Iter 88000, Minibatch Loss= 0.649587, Training Accuracy= 0.77900\n",
      "Iter 89000, Minibatch Loss= 0.634088, Training Accuracy= 0.78200\n",
      "Iter 90000, Minibatch Loss= 0.605425, Training Accuracy= 0.79200\n",
      "Iter 91000, Minibatch Loss= 0.600853, Training Accuracy= 0.80100\n",
      "Iter 92000, Minibatch Loss= 0.622425, Training Accuracy= 0.78400\n",
      "Iter 93000, Minibatch Loss= 0.610427, Training Accuracy= 0.79800\n",
      "Iter 94000, Minibatch Loss= 0.628229, Training Accuracy= 0.77000\n",
      "Iter 95000, Minibatch Loss= 0.599019, Training Accuracy= 0.80000\n",
      "Iter 96000, Minibatch Loss= 0.613960, Training Accuracy= 0.80300\n",
      "Iter 97000, Minibatch Loss= 0.640073, Training Accuracy= 0.76400\n",
      "Iter 98000, Minibatch Loss= 0.613107, Training Accuracy= 0.78200\n",
      "Iter 99000, Minibatch Loss= 0.632347, Training Accuracy= 0.77900\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "\n",
    "\n",
    "'''\n",
    "To classify images using a recurrent neural network, we consider every image\n",
    "row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
    "handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "sentence_size = 150\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 10 # MNIST data input (img shape: 28*28)\n",
    "n_steps = sentence_size/n_input # timesteps\n",
    "n_hidden = 20 # hidden layer num of features\n",
    "n_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    with tf.variable_scope(\"first_lstm63\"):\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "  \n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "#     return tf.nn.softmax(tf.matmul(outputs[-1], weights['out']) + biases['out'])\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "'''''''''\n",
    "    preds = tf.nn.softmax(logits)\n",
    "    correct = tf.equal(tf.cast(tf.argmax(preds,1),tf.int32), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    " '''''' '''  \n",
    "    \n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    iters=[]\n",
    "    costs=[]\n",
    "    accs=[]\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y_1 = PaddedDataIterator(train_df).next_batch(batch_size,sentence_size)\n",
    "        batch_y = np.concatenate((batch_y_1.reshape([-1,1]), 1-batch_y_1.reshape([-1,1])), axis=1)\n",
    "\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            iters.append(step*batch_size)\n",
    "            costs.append(loss)\n",
    "            accs.append(acc)\n",
    "            \n",
    "#             print( sess.run(pred, feed_dict={x: batch_x, y: batch_y}))\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "#     test_len = 128\n",
    "#     test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "#     test_label = mnist.test.labels[:test_len]\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAENCAYAAAAc1VI3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCYOIiFRhwiAiIlWYMIiISJX/Dwywv/kJi7VxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5d5d0bb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAENCAYAAAAc1VI3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd5d4f6bbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple demo of a scatter plot.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#     accs=[]\n",
    "    \n",
    "x = iters\n",
    "y = costs\n",
    "z = accs\n",
    "\n",
    "plt.scatter(x, y )\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cost')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x, z)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,  176,   34,   33,   55,  473,    5,  371,   68,  111,    4,\n",
       "        2523,  133,    5,   34,  371,    3,  140,  539,    2,   55,    1,\n",
       "        2409,   22,    1,   64,  210,   20,  237,    5,  175,  335,   21,\n",
       "           5,  193,  271,    5, 1097,  326,   53,    1,  216,   57,    9,\n",
       "        1349,    1,   17,   10,  150,   15,  266,    4,  213,   31, 2480,\n",
       "           9,  849,    1,   17,   15, 2183, 1508,  426,   11,   15,  352,\n",
       "          13, 2397,  432,    4,    3,  179, 1624,   40,    0, 2087,  663,\n",
       "           4,  891,  819,    2,  215,  192,   11,  512,    0,  266,  822,\n",
       "           7,    3,   62,  207,   23,    1,   43,    2,  612,    5,  125,\n",
       "          23],\n",
       "       [   1,  182,   10,   91,    3,   62,  100,    3,  160,  329,  136,\n",
       "          12,    1,   78,  451,   13,    5,    1,  263,    2,  968,    5,\n",
       "           1,  124,    5,   16, 1273,    4, 1381,   36,   43,  131, 2129,\n",
       "        1394,    0, 1723,    4,   17,  101,  692,    0, 1882,   11,    0,\n",
       "        1361, 2415, 2062, 1912,   16,    0,  732,    5,  839,   12,    0,\n",
       "         695,    7,   20, 1188,   23,    6,  220,  968,    5,   21,    3,\n",
       "         110,  423,   99,  148,  451,   13, 2383, 2179,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 182,   10,  242,    8,   15,  194,    2,  313,  123,  601,  563,\n",
       "          11,    4,    5,  389,   14,    0,  538,    9,    0,  173,  563,\n",
       "         111,    0, 1992,  551,  882,  125,  638,    1,   72,    5,   41,\n",
       "          33,  251,   22,    6,   60,  124,  327,  421,    0,  242,   14,\n",
       "          22,    6,  112,    2,  124,  563,   11,   92,   40,   15,  194,\n",
       "          47,    6,   41,   17,    2, 2361,   16,   24,  165,  551,   26,\n",
       "         954,  429,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  10,   39, 1157, 1910,    4,  265, 2389,  125,    2,    3, 1613,\n",
       "         234, 1267,    7, 1455,    2,   35,    3,  179, 1495,   52,  169,\n",
       "        2577,   11, 2478, 2181, 1830,    4,   79,    3, 1563, 1538,   26,\n",
       "        1244,   27,  159,  440,  159,   14, 1975,   22,    6,  112,    3,\n",
       "          62,   50,  817,  130,   13,    0, 2066,  146, 1874,  196,   12,\n",
       "          63,  905,    0,  519,    7,    3,  440, 1450,   99,   51, 1502,\n",
       "         629,    9,    3, 1015,   39,   13,    3,  519,  196,    0,  449,\n",
       "           7,  553, 2402,  168,    0,  151, 1632,  114, 2212,    4, 2238,\n",
       "         198,    0,   65,   96,  410,    9,  121,    5,  212,  435,   14,\n",
       "          27],\n",
       "       [   1,  182,   10,  103,    2,  406,   96,  427,  171,  252,  370,\n",
       "          97,  461,   16,  439,  128,   34,  114,  472,    4,    0,  103,\n",
       "          18,    3,  493, 1120,   15,  427,  104,   20,   17,   31,  609,\n",
       "          23,   10, 1319,  103,  104,   27,  338,   68,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 478, 1983,   18,   42,    1, 2514,  365,    4,   45,  238, 2338,\n",
       "          78,  589,   13,    0, 2435,  155, 2322,  194,  566,    5,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  74,  238,    1,  114, 1981,   42,    1,  328,    4,  492,    5,\n",
       "          11,  161,  784,    1, 1291,    4,   48,    2,   15,  767,    4,\n",
       "         533,   21,   68,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 110,   10,   16,  333,  546,  439, 1131,   52, 1845,   97,   24,\n",
       "         663,   26,  277,    8,  195,   26, 1115,    1,  745,  205,    5,\n",
       "         389,  209,  213,   24,  549,   43,   15,  195,  405,  860,    4,\n",
       "         426,   11,    5,  252,  225,   97,    0,  461,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [   2,   15,   80,    9, 1787,   92,   19,   49,   96, 2589,  142,\n",
       "         215,    0,  579,   52, 1646,    3,  531, 2544,  404,  120,    7,\n",
       "           0,   39,  233,    5,    7,   20,   32,  391,    5,  249,   79,\n",
       "         500,   27,  111,   57,    9,   32,  190,  845,  642,    1,  165,\n",
       "         301,  211,    4,   79,    0, 2473,  500,  233,  422,  559,   37,\n",
       "           0, 2317,    9,    3,  815,   14,   10,   31,  534,    5,    2,\n",
       "           3,  381,  146,   77,  239,   33,  878,    2, 1862,   76,  380,\n",
       "           8, 1081,    5,  273, 1316,    7,    0,  465,   12,  598,    0,\n",
       "        1483,    9,  138,  503,  880,    2,  582, 1431,  812, 1533,  450,\n",
       "          10],\n",
       "       [  10,   18,    3,   32,  446,  426,   12,    1,  110,   11, 2123,\n",
       "         243,    7, 1373, 2559,   23,    5,  193,   20,  435,   97,    3,\n",
       "         549,   14,  268,    2,  157,    3, 1758,   26,    3, 1498,    8,\n",
       "         575, 2345,  591,  877,  574,  244, 2572,   30,  190,  313, 1213,\n",
       "        1776, 2340,   13, 1249,   30,  313, 1948, 1077,   26,  917,   22,\n",
       "           6, 1865,  207,  266,  103,   12,   30,   33, 1310,   59, 1795,\n",
       "           3,  681,  426,   11, 2225,  649,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]], dtype=int32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)\n",
    "batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "\n",
    "x = tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_5:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:1' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:2' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:3' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:4' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:5' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:6' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:7' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:8' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:9' shape=(?, 10) dtype=float32>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define variables \n",
    "n_hidden = 10 \n",
    "with tf.variable_scope(\"softmax\"):\n",
    "    W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "    b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "\n",
    "with tf.variable_scope(\"recurrent\"):\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    initial_h_ = self.cell_.zero_state(self.batch_size_,dtype=tf.float32)\n",
    "    outputs, self.final_h_ = tf.nn.dynamic_rnn(self.cell_, inputs=self.x_,initial_state=self.initial_h_, sequence_length=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for _ in xrange(num_layers):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(H, forget_bias=0.0)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "        cells.append(cell)\n",
    "    return tf.contrib.rnn.MultiRNNCell(cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##have to fix here!!!\n",
    "def RNN(x):\n",
    "    n_hidden = 10 \n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "        b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "    # reshape to [1, n_input]\n",
    "    n_input = len(x)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "#     print x\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "#     with tf.variable_scope('cell_def'):\n",
    "#         lstm_cell = MakeFancyRNNCell(H=n_hidden,keep_prob=1)\n",
    "    lstm_cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(n_hidden, forget_bias=0.0)\n",
    "#     print x\n",
    "    # generate prediction\n",
    "\n",
    "    \n",
    "#     with tf.variable_scope('rnn_def',reuse=True):\n",
    "    initial_h_ = lstm_cell.zero_state(1,dtype=tf.int32)\n",
    "    outputs,final_h_ = tf.contrib.rnn.static_rnn(lstm_cell, x,dtype=tf.int32)\n",
    "            \n",
    "            \n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], W_out_) + b_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'static_rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-320bf1dbf5de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sample_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-c600588cd114>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     with tf.variable_scope('rnn_def',reuse=True):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'static_rnn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "learning_rate =0.1\n",
    "pred = RNN(X_sample_ids[0])\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_sample[0]))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, loss, onehot_pred = session.run([optimizer, cost, pred], \n",
    "                                        feed_dict={x: symbols_in_keys, y: symbols_out_onehot})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60843,)\n",
      "(26177,)\n",
      "(60843, 1)\n",
      "(26177, 1)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sample = nonnan_doc_clean[0:100]\n",
    "y_sample = y[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "print len(X_sample[0].split())\n",
    "print X_sample[0]\n",
    "print y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.__getattr__('brown').words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = X_sample[0].split()\n",
    "bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['For' 'what' 'I' 'paid' 'for' 'two' 'tutus' 'is' 'unbeatable' 'anywhere!'\n",
      "  'I' 'ordered' 'a' 'pink' 'and' 'turquios' 'and' 'they' 'are' 'vibrant'\n",
      "  'and' 'beautiful!' 'The' 'tutu' 'is' 'very' 'full!' 'Princess' 'style!'\n",
      "  'Not' 'cheaply' 'made!' 'Not' 'cheap' 'materia!' 'Obviously' 'someone'\n",
      "  'made' 'these' 'with' 'love' 'and' 'care!' 'I' 'paid' 'less' 'than' '7'\n",
      "  'bucks' 'for' 'a' 'tutu' 'I' 'and' 'I' 'feel' 'proud' 'of' 'my' 'self'\n",
      "  'for' 'researching' 'to' 'the' 'point' 'of' 'finding' 'gold!Recommend'\n",
      "  '2-6' 'years!My' 'daughter' 'is' 'two' '!' 'Wears' 'size' '4t' 'and'\n",
      "  'this' 'skirt' '(' 'one' 'size' ')' 'fit' 'perfect' 'and' 'will'\n",
      "  'probaly' 'be' 'able' 'to' 'accommodate' 'her' 'quickly' 'growing'\n",
      "  'waist' 'for' 'some']]\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "for i, (w,y) in enumerate(bi):\n",
    "    print w\n",
    "    print y_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "# reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.learning_rate_:learning_rate,\n",
    "                 lm.initial_h_ : h}\n",
    "            \n",
    "        cost, _, h = session.run([loss, train_op,lm.final_h_],\n",
    "                       feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V=1000\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=100, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "# TF_SAVEDIR = \"tf_saved\"\n",
    "# checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "# trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-576b79a3ba51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnnlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildCoreGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildTrainGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mBuildCoreGraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((input_,))\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   3719\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m   \"\"\"\n\u001b[0;32m-> 3721\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3722\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3]."
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "# shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "# if not os.path.isdir(TF_SAVEDIR):\n",
    "#     os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "#         bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "        corpus = X_sample[0].split()\n",
    "        bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)\n",
    "        for i, (w,y) in enumerate(bi):\n",
    "            w1=w\n",
    "            \n",
    "        bi= (w1,y_sample[y])\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=learning_rate, train=True, \n",
    "                     verbose=False, tick_s=3600)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/legu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "corpus = nltk.corpus.brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vocabulary\n",
      "  Downloading Vocabulary-1.0.4.tar.gz\n",
      "Collecting requests==2.13.0 (from vocabulary)\n",
      "  Downloading requests-2.13.0-py2.py3-none-any.whl (584kB)\n",
      "\u001b[K    100% || 593kB 408kB/s \n",
      "\u001b[?25hCollecting mock==2.0.0 (from vocabulary)\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% || 61kB 1.9MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): funcsigs>=1; python_version < \"3.3\" in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.9 in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Collecting pbr>=0.11 (from mock==2.0.0->vocabulary)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% || 102kB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: vocabulary\n",
      "  Running setup.py bdist_wheel for vocabulary ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/legu/Library/Caches/pip/wheels/36/6c/c0/92bb20f79402d055c3bce3e89d9f2cce5d6937bc2aadc0fb45\n",
      "Successfully built vocabulary\n",
      "Installing collected packages: requests, pbr, mock, vocabulary\n",
      "  Found existing installation: requests 2.11.1\n",
      "    Uninstalling requests-2.11.1:\n",
      "      Successfully uninstalled requests-2.11.1\n",
      "Successfully installed mock-2.0.0 pbr-3.1.1 requests-2.13.0 vocabulary-1.0.4\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'Vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-385fd0cf2b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Train set vocabulary: %d words\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'Vocabulary'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import vocabulary\n",
    "# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\n",
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in utils.flatten(corpus))\n",
    "print \"Train set vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
