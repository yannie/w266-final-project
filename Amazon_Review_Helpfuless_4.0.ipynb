{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Sklearn libraries.\n",
    "from sklearn import datasets, linear_model, ensemble, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "#deep learning library\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, tf_embed_viz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip gz file.\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "# Load JSON into dataframe.\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Used 40.319883s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = getDF('/home/legu/data/reviews_Clothing_Shoes_and_Jewelry_5.json.gz')\n",
    "end = time.time()\n",
    "print \"Time Used %fs\" %(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'reviewerID', u'asin', u'reviewerName', u'helpful', u'unixReviewTime',\n",
      "       u'reviewText', u'overall', u'reviewTime', u'summary'],\n",
      "      dtype='object')\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1KLRMWW2FWPL4</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer \"cameramom\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1297468800</td>\n",
       "      <td>This is a great tutu and at a really great pri...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>02 12, 2011</td>\n",
       "      <td>Great tutu-  not cheaply made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2G5TCU2WDFZ65</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1358553600</td>\n",
       "      <td>I bought this for my 4 yr old daughter for dan...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 19, 2013</td>\n",
       "      <td>Very Cute!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1RLQXYNCMWRWN</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Carola</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1357257600</td>\n",
       "      <td>What can I say... my daughters have it in oran...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 4, 2013</td>\n",
       "      <td>I have buy more than one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A8U3FAMSJVHS5</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Caromcg</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1398556800</td>\n",
       "      <td>We bought several tutus at once, and they are ...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>04 27, 2014</td>\n",
       "      <td>Adorable, Sturdy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3GEOILWLK86XM</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>CJ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1394841600</td>\n",
       "      <td>Thank you Halo Heaven great product for Little...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>Grammy's Angels Love it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                 reviewerName helpful  \\\n",
       "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
       "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
       "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
       "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
       "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
       "\n",
       "   unixReviewTime                                         reviewText  overall  \\\n",
       "0      1297468800  This is a great tutu and at a really great pri...   5.0000   \n",
       "1      1358553600  I bought this for my 4 yr old daughter for dan...   5.0000   \n",
       "2      1357257600  What can I say... my daughters have it in oran...   5.0000   \n",
       "3      1398556800  We bought several tutus at once, and they are ...   5.0000   \n",
       "4      1394841600  Thank you Halo Heaven great product for Little...   5.0000   \n",
       "\n",
       "    reviewTime                        summary  \n",
       "0  02 12, 2011  Great tutu-  not cheaply made  \n",
       "1  01 19, 2013                    Very Cute!!  \n",
       "2   01 4, 2013       I have buy more than one  \n",
       "3  04 27, 2014               Adorable, Sturdy  \n",
       "4  03 15, 2014        Grammy's Angels Love it  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.columns\n",
    "\n",
    "print df['reviewText'][0]\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring helpfulness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helpfulness = []\n",
    "total_votes = []\n",
    "for i in df['helpful']:\n",
    "    if i[1] == 0:\n",
    "        helpfulness.append(np.nan)\n",
    "        total_votes.append(np.nan)\n",
    "    else:\n",
    "        helpfulness.append(float(i[0])/i[1])\n",
    "        total_votes.append(i[1])\n",
    "        \n",
    "# Convert to numpy array.\n",
    "helpfulness = np.array(helpfulness)\n",
    "total_votes = np.array(total_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot boxplot.\n",
    "nonnan_helpfulness = helpfulness[~np.isnan(helpfulness)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove invalid and nan values.\n",
    "helpfulness_clean = np.delete(helpfulness, 30730)\n",
    "\n",
    "nonnan_helpfulness_clean = helpfulness_clean[~np.isnan(helpfulness_clean)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract the review text\n",
    "doc = np.array(df['reviewText'])\n",
    "\n",
    "# Filter down to reviews with helpfulness scores.\n",
    "# Remove item with invalid helpfulness score.\n",
    "doc_clean = np.delete(doc, 30730)\n",
    "nonnan_doc_clean = doc_clean[~np.isnan(helpfulness_clean)]\n",
    "y = np.reshape(nonnan_helpfulness_clean,(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep Learning Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Split the dataset into traing and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(nonnan_doc_clean )\n",
    "msk = np.random.rand(nonnan_doc_clean.shape[0]) <= 0.7\n",
    "\n",
    "X_train = nonnan_doc_clean[msk]\n",
    "X_test = nonnan_doc_clean[~msk]\n",
    "y_train = y[msk]\n",
    "y_test = y[~msk]\n",
    "X_sample = nonnan_doc_clean[0:60000]\n",
    "y_sample = y[0:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For what I paid for two tutus is unbeatable anywhere!  I ordered a pink and turquios and they are vibrant and beautiful! The tutu is very full! Princess style! Not cheaply made! Not cheap materia! Obviously someone made these with love and care! I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two ! Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61005\n",
      "26015\n",
      "26015\n"
     ]
    }
   ],
   "source": [
    "print (len(X_train))\n",
    "print (len(X_test))\n",
    "print (len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Convert the review from list of words to list of word dictionary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(docs):\n",
    "    tokens =[]\n",
    "    for i in docs:\n",
    "        lowers = i.lower()\n",
    "        #remove the punctuation using the character deletion step of translate\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token = nltk.word_tokenize(no_punctuation)\n",
    "        tokens.extend(token)\n",
    "    return tokens\n",
    "\n",
    "tokens = get_tokens(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic, rev_dict = build_dataset(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_dict[1461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build sample data\n",
    "X_sample_ids = []\n",
    "for i in X_sample:\n",
    "    lowers = i.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token = nltk.word_tokenize(no_punctuation)\n",
    "    j=[dic[w] for w in token]\n",
    "    X_sample_ids.append(j)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build test data, note if word not in the dictionary, ignore\n",
    "X_test_ids = []\n",
    "for i in X_test:\n",
    "    lowers = i.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token = nltk.word_tokenize(no_punctuation)\n",
    "    j=[dic.get(w) for w in token if dic.get(w) is not None]   \n",
    "    X_test_ids.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For what I paid for two tutus is unbeatable anywhere!  I ordered a pink and turquios and they are vibrant and beautiful! The tutu is very full! Princess style! Not cheaply made! Not cheap materia! Obviously someone made these with love and care! I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two ! Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!\n",
      "[8, 86, 1, 611, 8, 134, 13856, 6, 5996, 1122, 1, 84, 3, 567, 2, 36860, 2, 12, 13, 1811, 2, 255, 0, 6132, 6, 26, 440, 3268, 194, 19, 1510, 90, 19, 259, 80013, 1183, 479, 90, 17, 20, 55, 2, 623, 1, 611, 326, 52, 415, 1225, 8, 3, 6132, 1, 2, 1, 115, 3151, 7, 11, 1969, 8, 5697, 4, 0, 580, 7, 813, 78856, 3345, 39652, 414, 6, 134, 460, 29, 6679, 2, 10, 615, 37, 29, 32, 96, 2, 42, 76260, 28, 338, 4, 1768, 206, 473, 2884, 225, 8, 87, 76]\n",
      "[ 0.875]\n",
      "Full and well stitched.  This tutu is a beautiful purple color that looks just like the picture.  It looks just adorable on our little fairy.\n",
      "[440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 92, 14, 105, 33, 27, 0, 289, 5, 105, 33, 1188, 16, 455, 65, 10064]\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "print (X_sample[0])\n",
    "print (X_sample_ids[0])\n",
    "print (y_sample[0])\n",
    "\n",
    "print (X_test[0])\n",
    "print (X_test_ids[0])\n",
    "print (y_test[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Pad the sentence with differnt length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine the list of words index and label into a dataframe\n",
    "train_df = pd.DataFrame(np.column_stack([X_sample_ids,y_sample]), \n",
    "                               columns=['list_words', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 86, 1, 611, 8, 134, 13856, 6, 5996, 1122, ...</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48...</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[55, 10, 494, 270, 22, 46, 141, 661, 610, 8284...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          list_words  label\n",
       "0  [8, 86, 1, 611, 8, 134, 13856, 6, 5996, 1122, ... 0.8750\n",
       "1  [440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9... 1.0000\n",
       "2  [3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48... 0.5000\n",
       "3  [0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ... 1.0000\n",
       "4  [55, 10, 494, 270, 22, 46, 141, 661, 610, 8284... 0.0000"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine the list of words index and label into a dataframe\n",
    "test_df = pd.DataFrame(np.column_stack([X_test_ids,y_test]), \n",
    "                               columns=['list_words', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48...</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[10, 627, 6, 38, 8, 426, 20, 3, 164, 7, 494, 1...</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 3597, 860, 1571, 645, 23, 7944, 226, 86, 1...</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[48, 9, 0, 1815, 1, 24, 18154, 9, 1999, 8, 272...</td>\n",
       "      <td>0.4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          list_words  label\n",
       "0  [440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9... 1.0000\n",
       "1  [3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48... 0.5000\n",
       "2  [10, 627, 6, 38, 8, 426, 20, 3, 164, 7, 494, 1... 0.5000\n",
       "3  [0, 3597, 860, 1571, 645, 23, 7944, 226, 86, 1... 0.7778\n",
       "4  [48, 9, 0, 1815, 1, 24, 18154, 9, 1999, 8, 272... 0.4000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [1, 27, 17, 85, 43, 52, 0, 11071, 1958, 1, 84,...\n",
      "1    [480, 39918, 2097, 136, 4, 2447, 443, 219, 88,...\n",
      "2    [1, 56, 8015, 3, 671, 14, 83841, 11, 2111, 20,...\n",
      "Name: list_words, dtype: object\n",
      "0   1.0000\n",
      "1   1.0000\n",
      "2   1.0000\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "##this is not necessary\n",
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "        return res['list_words'], res['label'] \n",
    "    \n",
    "data = SimpleDataIterator(train_df)\n",
    "a,b = data.next_batch(3)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##this helps to make sure each sentence have the same length (max_len)\n",
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n, max_len):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "#             self.max_len = max_len\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        maxlen = max_len\n",
    "        x = np.zeros([n, maxlen], dtype=np.int32)\n",
    "        for i, x_i in enumerate(x):\n",
    "            l=len(res['list_words'].values[i]) ##list length\n",
    "            if l>maxlen: \n",
    "                x_i[:maxlen] = res['list_words'].values[i][:max_len]\n",
    "            else:\n",
    "                x_i[:l] = res['list_words'].values[i][:l]\n",
    "\n",
    "        return x, res['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pad = PaddedDataIterator(train_df)\n",
    "data_pad_test = PaddedDataIterator(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1   55   10  637  505  108  166    3  171    7  190  145  116    2\n",
      "    55   21   41    0 2340   16   87    7   21   13 1292   15    0   88\n",
      "     6  595    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "0   1.0000\n",
      "Name: label, dtype: object\n",
      "[[  12 1845   68 1349    2  789   68  600   12   13   90    7   57   69\n",
      "  1258    2   22   54   28  195    9    3 1807 2810   12  128   20  344\n",
      "     1   57   55   21    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "0   0.5000\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#sample call to get the data\n",
    "max_length = 100 \n",
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(1,max_length)\n",
    "batch_x_test, batch_y_test = PaddedDataIterator(test_df).next_batch(1,max_length)\n",
    "print (batch_x)\n",
    "print (batch_y)\n",
    "print (batch_x_test)\n",
    "print (batch_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.build RNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:100: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000, Minibatch Loss= 0.931417, Training Accuracy= 0.26800\n",
      "Iter 2000, Minibatch Loss= 0.940382, Training Accuracy= 0.25300\n",
      "Iter 3000, Minibatch Loss= 0.941001, Training Accuracy= 0.25000\n",
      "Iter 4000, Minibatch Loss= 0.902102, Training Accuracy= 0.27500\n",
      "Iter 5000, Minibatch Loss= 0.891506, Training Accuracy= 0.26500\n",
      "Iter 6000, Minibatch Loss= 0.889545, Training Accuracy= 0.26900\n",
      "Iter 7000, Minibatch Loss= 0.866880, Training Accuracy= 0.28600\n",
      "Iter 8000, Minibatch Loss= 0.891438, Training Accuracy= 0.25000\n",
      "Iter 9000, Minibatch Loss= 0.853439, Training Accuracy= 0.30100\n",
      "Iter 10000, Minibatch Loss= 0.846970, Training Accuracy= 0.29900\n",
      "Iter 11000, Minibatch Loss= 0.844638, Training Accuracy= 0.30500\n",
      "Iter 12000, Minibatch Loss= 0.815916, Training Accuracy= 0.33200\n",
      "Iter 13000, Minibatch Loss= 0.794854, Training Accuracy= 0.33400\n",
      "Iter 14000, Minibatch Loss= 0.795694, Training Accuracy= 0.34200\n",
      "Iter 15000, Minibatch Loss= 0.773681, Training Accuracy= 0.36200\n",
      "Iter 16000, Minibatch Loss= 0.780776, Training Accuracy= 0.36000\n",
      "Iter 17000, Minibatch Loss= 0.757538, Training Accuracy= 0.37300\n",
      "Iter 18000, Minibatch Loss= 0.762160, Training Accuracy= 0.38600\n",
      "Iter 19000, Minibatch Loss= 0.734666, Training Accuracy= 0.41500\n",
      "Iter 20000, Minibatch Loss= 0.716573, Training Accuracy= 0.44400\n",
      "Iter 21000, Minibatch Loss= 0.711524, Training Accuracy= 0.48200\n",
      "Iter 22000, Minibatch Loss= 0.716826, Training Accuracy= 0.49200\n",
      "Iter 23000, Minibatch Loss= 0.686421, Training Accuracy= 0.54100\n",
      "Iter 24000, Minibatch Loss= 0.704492, Training Accuracy= 0.53800\n",
      "Iter 25000, Minibatch Loss= 0.672911, Training Accuracy= 0.59000\n",
      "Iter 26000, Minibatch Loss= 0.658514, Training Accuracy= 0.62900\n",
      "Iter 27000, Minibatch Loss= 0.645824, Training Accuracy= 0.67300\n",
      "Iter 28000, Minibatch Loss= 0.644501, Training Accuracy= 0.67800\n",
      "Iter 29000, Minibatch Loss= 0.637273, Training Accuracy= 0.70500\n",
      "Iter 30000, Minibatch Loss= 0.616452, Training Accuracy= 0.73700\n",
      "Iter 31000, Minibatch Loss= 0.621278, Training Accuracy= 0.75100\n",
      "Iter 32000, Minibatch Loss= 0.597906, Training Accuracy= 0.77300\n",
      "Iter 33000, Minibatch Loss= 0.597016, Training Accuracy= 0.76600\n",
      "Iter 34000, Minibatch Loss= 0.596783, Training Accuracy= 0.76900\n",
      "Iter 35000, Minibatch Loss= 0.602212, Training Accuracy= 0.78800\n",
      "Iter 36000, Minibatch Loss= 0.608001, Training Accuracy= 0.78300\n",
      "Iter 37000, Minibatch Loss= 0.600461, Training Accuracy= 0.78800\n",
      "Iter 38000, Minibatch Loss= 0.606196, Training Accuracy= 0.78200\n",
      "Iter 39000, Minibatch Loss= 0.632137, Training Accuracy= 0.78200\n",
      "Iter 40000, Minibatch Loss= 0.607089, Training Accuracy= 0.77700\n",
      "Iter 41000, Minibatch Loss= 0.579795, Training Accuracy= 0.82200\n",
      "Iter 42000, Minibatch Loss= 0.575120, Training Accuracy= 0.81200\n",
      "Iter 43000, Minibatch Loss= 0.573341, Training Accuracy= 0.82000\n",
      "Iter 44000, Minibatch Loss= 0.599143, Training Accuracy= 0.81700\n",
      "Iter 45000, Minibatch Loss= 0.602557, Training Accuracy= 0.80900\n",
      "Iter 46000, Minibatch Loss= 0.604408, Training Accuracy= 0.79400\n",
      "Iter 47000, Minibatch Loss= 0.585909, Training Accuracy= 0.80900\n",
      "Iter 48000, Minibatch Loss= 0.556823, Training Accuracy= 0.83200\n",
      "Iter 49000, Minibatch Loss= 0.603165, Training Accuracy= 0.79800\n",
      "Iter 50000, Minibatch Loss= 0.578970, Training Accuracy= 0.80400\n",
      "Iter 51000, Minibatch Loss= 0.580756, Training Accuracy= 0.82200\n",
      "Iter 52000, Minibatch Loss= 0.556957, Training Accuracy= 0.83900\n",
      "Iter 53000, Minibatch Loss= 0.565530, Training Accuracy= 0.82300\n",
      "Iter 54000, Minibatch Loss= 0.576549, Training Accuracy= 0.81000\n",
      "Iter 55000, Minibatch Loss= 0.536255, Training Accuracy= 0.83900\n",
      "Iter 56000, Minibatch Loss= 0.570653, Training Accuracy= 0.81300\n",
      "Iter 57000, Minibatch Loss= 0.550237, Training Accuracy= 0.82800\n",
      "Iter 58000, Minibatch Loss= 0.561397, Training Accuracy= 0.82300\n",
      "Iter 59000, Minibatch Loss= 0.555032, Training Accuracy= 0.83900\n",
      "Iter 60000, Minibatch Loss= 0.573507, Training Accuracy= 0.82700\n",
      "Iter 61000, Minibatch Loss= 0.551024, Training Accuracy= 0.83100\n",
      "Iter 62000, Minibatch Loss= 0.576448, Training Accuracy= 0.82500\n",
      "Iter 63000, Minibatch Loss= 0.573952, Training Accuracy= 0.80600\n",
      "Iter 64000, Minibatch Loss= 0.583966, Training Accuracy= 0.81400\n",
      "Iter 65000, Minibatch Loss= 0.561372, Training Accuracy= 0.82600\n",
      "Iter 66000, Minibatch Loss= 0.588686, Training Accuracy= 0.82200\n",
      "Iter 67000, Minibatch Loss= 0.550429, Training Accuracy= 0.82800\n",
      "Iter 68000, Minibatch Loss= 0.571007, Training Accuracy= 0.81900\n",
      "Iter 69000, Minibatch Loss= 0.569601, Training Accuracy= 0.82000\n",
      "Iter 70000, Minibatch Loss= 0.519164, Training Accuracy= 0.85500\n",
      "Iter 71000, Minibatch Loss= 0.551613, Training Accuracy= 0.83400\n",
      "Iter 72000, Minibatch Loss= 0.563147, Training Accuracy= 0.82300\n",
      "Iter 73000, Minibatch Loss= 0.556735, Training Accuracy= 0.83200\n",
      "Iter 74000, Minibatch Loss= 0.553031, Training Accuracy= 0.83100\n",
      "Iter 75000, Minibatch Loss= 0.565023, Training Accuracy= 0.81000\n",
      "Iter 76000, Minibatch Loss= 0.538806, Training Accuracy= 0.84500\n",
      "Iter 77000, Minibatch Loss= 0.556477, Training Accuracy= 0.84100\n",
      "Iter 78000, Minibatch Loss= 0.574180, Training Accuracy= 0.81700\n",
      "Iter 79000, Minibatch Loss= 0.567660, Training Accuracy= 0.82500\n",
      "Iter 80000, Minibatch Loss= 0.555128, Training Accuracy= 0.82900\n",
      "Iter 81000, Minibatch Loss= 0.564096, Training Accuracy= 0.83000\n",
      "Iter 82000, Minibatch Loss= 0.543484, Training Accuracy= 0.85100\n",
      "Iter 83000, Minibatch Loss= 0.578429, Training Accuracy= 0.79500\n",
      "Iter 84000, Minibatch Loss= 0.551320, Training Accuracy= 0.83800\n",
      "Iter 85000, Minibatch Loss= 0.531805, Training Accuracy= 0.83700\n",
      "Iter 86000, Minibatch Loss= 0.539410, Training Accuracy= 0.84000\n",
      "Iter 87000, Minibatch Loss= 0.571830, Training Accuracy= 0.81100\n",
      "Iter 88000, Minibatch Loss= 0.544718, Training Accuracy= 0.83800\n",
      "Iter 89000, Minibatch Loss= 0.563417, Training Accuracy= 0.80800\n",
      "Iter 90000, Minibatch Loss= 0.571106, Training Accuracy= 0.82200\n",
      "Iter 91000, Minibatch Loss= 0.532051, Training Accuracy= 0.85200\n",
      "Iter 92000, Minibatch Loss= 0.552093, Training Accuracy= 0.83100\n",
      "Iter 93000, Minibatch Loss= 0.567971, Training Accuracy= 0.81500\n",
      "Iter 94000, Minibatch Loss= 0.554480, Training Accuracy= 0.82200\n",
      "Iter 95000, Minibatch Loss= 0.557926, Training Accuracy= 0.83200\n",
      "Iter 96000, Minibatch Loss= 0.535081, Training Accuracy= 0.83400\n",
      "Iter 97000, Minibatch Loss= 0.538526, Training Accuracy= 0.84700\n",
      "Iter 98000, Minibatch Loss= 0.546775, Training Accuracy= 0.84100\n",
      "Iter 99000, Minibatch Loss= 0.534848, Training Accuracy= 0.84300\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:123: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.834134\n",
      "Testing cost: 0.556403\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "\n",
    "\n",
    "'''\n",
    "To classify images using a recurrent neural network, we consider every image\n",
    "row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
    "handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "######################## \n",
    "### MODEL PARAMETERS ###\n",
    "######################## \n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "sentence_size = 150\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 10  \n",
    "n_steps = sentence_size/n_input # timesteps\n",
    "n_hidden = 20 # hidden layer num of features\n",
    "n_classes = 2  \n",
    "\n",
    "\n",
    "#################################\n",
    "### PLACEHOLDER AND VARIABLE ###\n",
    "#################################\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "######################## \n",
    "### DEFINE RNN MODEL ###\n",
    "######################## \n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    \n",
    "    with tf.variable_scope(\"first_lstm66\"):\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "### LOSS AND OPTIMIZATION ###\n",
    "#############################\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "##################\n",
    "### EVALUATION ###\n",
    "################## \n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "### RUN GRAPH ###\n",
    "#################\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    iters=[]\n",
    "    costs=[]\n",
    "    accs=[]\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        \n",
    "        ##########################\n",
    "        ### GENERATE BATCH X,Y ###\n",
    "        ##########################\n",
    "        batch_x, batch_y_1 = PaddedDataIterator(train_df).next_batch(batch_size,sentence_size)\n",
    "        batch_y = np.concatenate((batch_y_1.reshape([-1,1]), 1-batch_y_1.reshape([-1,1])), axis=1)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        \n",
    "        \n",
    "        ################\n",
    "        ### TRAINING ###\n",
    "        ################\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        #################\n",
    "        ### REPORTING ###\n",
    "        #################\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy, loss\n",
    "            acc,loss = sess.run([accuracy,cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            iters.append(step*batch_size)\n",
    "            costs.append(loss)\n",
    "            accs.append(acc)\n",
    "#             print( sess.run(pred, feed_dict={x: batch_x, y: batch_y}))\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    ##########################\n",
    "    ### EVALUATE TEST DATA ###\n",
    "    ##########################    \n",
    "    \n",
    "    batch_x_test_1, batch_y_1_test = PaddedDataIterator(test_df).next_batch(26015,sentence_size)\n",
    "    batch_x_test = batch_x_test_1.reshape([-1,n_steps, n_input])\n",
    "    batch_y_test = np.concatenate((batch_y_1_test.reshape([-1,1]), 1-batch_y_1_test.reshape([-1,1])), axis=1)\n",
    "    test_acc, test_cost = sess.run([accuracy, cost], feed_dict={x: batch_x_test, y: batch_y_test})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAENCAYAAAAc1VI3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0b8886c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAENCAYAAAAc1VI3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOOQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0b8890110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.834134\n",
      "Testing cost: 0.556403\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple demo of a scatter plot.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#     accs=[]\n",
    "    \n",
    "x = iters\n",
    "y = costs\n",
    "z = accs\n",
    "\n",
    "plt.scatter(x, y )\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cost')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x, z)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "print (\"Testing Accuracy:\",test_acc)\n",
    "print(\"Testing cost:\",test_cost)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,  176,   34,   33,   55,  473,    5,  371,   68,  111,    4,\n",
       "        2523,  133,    5,   34,  371,    3,  140,  539,    2,   55,    1,\n",
       "        2409,   22,    1,   64,  210,   20,  237,    5,  175,  335,   21,\n",
       "           5,  193,  271,    5, 1097,  326,   53,    1,  216,   57,    9,\n",
       "        1349,    1,   17,   10,  150,   15,  266,    4,  213,   31, 2480,\n",
       "           9,  849,    1,   17,   15, 2183, 1508,  426,   11,   15,  352,\n",
       "          13, 2397,  432,    4,    3,  179, 1624,   40,    0, 2087,  663,\n",
       "           4,  891,  819,    2,  215,  192,   11,  512,    0,  266,  822,\n",
       "           7,    3,   62,  207,   23,    1,   43,    2,  612,    5,  125,\n",
       "          23],\n",
       "       [   1,  182,   10,   91,    3,   62,  100,    3,  160,  329,  136,\n",
       "          12,    1,   78,  451,   13,    5,    1,  263,    2,  968,    5,\n",
       "           1,  124,    5,   16, 1273,    4, 1381,   36,   43,  131, 2129,\n",
       "        1394,    0, 1723,    4,   17,  101,  692,    0, 1882,   11,    0,\n",
       "        1361, 2415, 2062, 1912,   16,    0,  732,    5,  839,   12,    0,\n",
       "         695,    7,   20, 1188,   23,    6,  220,  968,    5,   21,    3,\n",
       "         110,  423,   99,  148,  451,   13, 2383, 2179,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 182,   10,  242,    8,   15,  194,    2,  313,  123,  601,  563,\n",
       "          11,    4,    5,  389,   14,    0,  538,    9,    0,  173,  563,\n",
       "         111,    0, 1992,  551,  882,  125,  638,    1,   72,    5,   41,\n",
       "          33,  251,   22,    6,   60,  124,  327,  421,    0,  242,   14,\n",
       "          22,    6,  112,    2,  124,  563,   11,   92,   40,   15,  194,\n",
       "          47,    6,   41,   17,    2, 2361,   16,   24,  165,  551,   26,\n",
       "         954,  429,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  10,   39, 1157, 1910,    4,  265, 2389,  125,    2,    3, 1613,\n",
       "         234, 1267,    7, 1455,    2,   35,    3,  179, 1495,   52,  169,\n",
       "        2577,   11, 2478, 2181, 1830,    4,   79,    3, 1563, 1538,   26,\n",
       "        1244,   27,  159,  440,  159,   14, 1975,   22,    6,  112,    3,\n",
       "          62,   50,  817,  130,   13,    0, 2066,  146, 1874,  196,   12,\n",
       "          63,  905,    0,  519,    7,    3,  440, 1450,   99,   51, 1502,\n",
       "         629,    9,    3, 1015,   39,   13,    3,  519,  196,    0,  449,\n",
       "           7,  553, 2402,  168,    0,  151, 1632,  114, 2212,    4, 2238,\n",
       "         198,    0,   65,   96,  410,    9,  121,    5,  212,  435,   14,\n",
       "          27],\n",
       "       [   1,  182,   10,  103,    2,  406,   96,  427,  171,  252,  370,\n",
       "          97,  461,   16,  439,  128,   34,  114,  472,    4,    0,  103,\n",
       "          18,    3,  493, 1120,   15,  427,  104,   20,   17,   31,  609,\n",
       "          23,   10, 1319,  103,  104,   27,  338,   68,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 478, 1983,   18,   42,    1, 2514,  365,    4,   45,  238, 2338,\n",
       "          78,  589,   13,    0, 2435,  155, 2322,  194,  566,    5,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  74,  238,    1,  114, 1981,   42,    1,  328,    4,  492,    5,\n",
       "          11,  161,  784,    1, 1291,    4,   48,    2,   15,  767,    4,\n",
       "         533,   21,   68,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 110,   10,   16,  333,  546,  439, 1131,   52, 1845,   97,   24,\n",
       "         663,   26,  277,    8,  195,   26, 1115,    1,  745,  205,    5,\n",
       "         389,  209,  213,   24,  549,   43,   15,  195,  405,  860,    4,\n",
       "         426,   11,    5,  252,  225,   97,    0,  461,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [   2,   15,   80,    9, 1787,   92,   19,   49,   96, 2589,  142,\n",
       "         215,    0,  579,   52, 1646,    3,  531, 2544,  404,  120,    7,\n",
       "           0,   39,  233,    5,    7,   20,   32,  391,    5,  249,   79,\n",
       "         500,   27,  111,   57,    9,   32,  190,  845,  642,    1,  165,\n",
       "         301,  211,    4,   79,    0, 2473,  500,  233,  422,  559,   37,\n",
       "           0, 2317,    9,    3,  815,   14,   10,   31,  534,    5,    2,\n",
       "           3,  381,  146,   77,  239,   33,  878,    2, 1862,   76,  380,\n",
       "           8, 1081,    5,  273, 1316,    7,    0,  465,   12,  598,    0,\n",
       "        1483,    9,  138,  503,  880,    2,  582, 1431,  812, 1533,  450,\n",
       "          10],\n",
       "       [  10,   18,    3,   32,  446,  426,   12,    1,  110,   11, 2123,\n",
       "         243,    7, 1373, 2559,   23,    5,  193,   20,  435,   97,    3,\n",
       "         549,   14,  268,    2,  157,    3, 1758,   26,    3, 1498,    8,\n",
       "         575, 2345,  591,  877,  574,  244, 2572,   30,  190,  313, 1213,\n",
       "        1776, 2340,   13, 1249,   30,  313, 1948, 1077,   26,  917,   22,\n",
       "           6, 1865,  207,  266,  103,   12,   30,   33, 1310,   59, 1795,\n",
       "           3,  681,  426,   11, 2225,  649,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]], dtype=int32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)\n",
    "batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "\n",
    "x = tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_5:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:1' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:2' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:3' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:4' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:5' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:6' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:7' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:8' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:9' shape=(?, 10) dtype=float32>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define variables \n",
    "n_hidden = 10 \n",
    "with tf.variable_scope(\"softmax\"):\n",
    "    W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "    b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "\n",
    "with tf.variable_scope(\"recurrent\"):\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    initial_h_ = self.cell_.zero_state(self.batch_size_,dtype=tf.float32)\n",
    "    outputs, self.final_h_ = tf.nn.dynamic_rnn(self.cell_, inputs=self.x_,initial_state=self.initial_h_, sequence_length=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for _ in xrange(num_layers):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(H, forget_bias=0.0)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "        cells.append(cell)\n",
    "    return tf.contrib.rnn.MultiRNNCell(cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##have to fix here!!!\n",
    "def RNN(x):\n",
    "    n_hidden = 10 \n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "        b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "    # reshape to [1, n_input]\n",
    "    n_input = len(x)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "#     print x\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "#     with tf.variable_scope('cell_def'):\n",
    "#         lstm_cell = MakeFancyRNNCell(H=n_hidden,keep_prob=1)\n",
    "    lstm_cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(n_hidden, forget_bias=0.0)\n",
    "#     print x\n",
    "    # generate prediction\n",
    "\n",
    "    \n",
    "#     with tf.variable_scope('rnn_def',reuse=True):\n",
    "    initial_h_ = lstm_cell.zero_state(1,dtype=tf.int32)\n",
    "    outputs,final_h_ = tf.contrib.rnn.static_rnn(lstm_cell, x,dtype=tf.int32)\n",
    "            \n",
    "            \n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], W_out_) + b_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'static_rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-320bf1dbf5de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sample_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-c600588cd114>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     with tf.variable_scope('rnn_def',reuse=True):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'static_rnn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "learning_rate =0.1\n",
    "pred = RNN(X_sample_ids[0])\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_sample[0]))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, loss, onehot_pred = session.run([optimizer, cost, pred], \n",
    "                                        feed_dict={x: symbols_in_keys, y: symbols_out_onehot})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60843,)\n",
      "(26177,)\n",
      "(60843, 1)\n",
      "(26177, 1)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sample = nonnan_doc_clean[0:100]\n",
    "y_sample = y[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "print len(X_sample[0].split())\n",
    "print X_sample[0]\n",
    "print y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.__getattr__('brown').words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = X_sample[0].split()\n",
    "bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['For' 'what' 'I' 'paid' 'for' 'two' 'tutus' 'is' 'unbeatable' 'anywhere!'\n",
      "  'I' 'ordered' 'a' 'pink' 'and' 'turquios' 'and' 'they' 'are' 'vibrant'\n",
      "  'and' 'beautiful!' 'The' 'tutu' 'is' 'very' 'full!' 'Princess' 'style!'\n",
      "  'Not' 'cheaply' 'made!' 'Not' 'cheap' 'materia!' 'Obviously' 'someone'\n",
      "  'made' 'these' 'with' 'love' 'and' 'care!' 'I' 'paid' 'less' 'than' '7'\n",
      "  'bucks' 'for' 'a' 'tutu' 'I' 'and' 'I' 'feel' 'proud' 'of' 'my' 'self'\n",
      "  'for' 'researching' 'to' 'the' 'point' 'of' 'finding' 'gold!Recommend'\n",
      "  '2-6' 'years!My' 'daughter' 'is' 'two' '!' 'Wears' 'size' '4t' 'and'\n",
      "  'this' 'skirt' '(' 'one' 'size' ')' 'fit' 'perfect' 'and' 'will'\n",
      "  'probaly' 'be' 'able' 'to' 'accommodate' 'her' 'quickly' 'growing'\n",
      "  'waist' 'for' 'some']]\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "for i, (w,y) in enumerate(bi):\n",
    "    print w\n",
    "    print y_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "# reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.learning_rate_:learning_rate,\n",
    "                 lm.initial_h_ : h}\n",
    "            \n",
    "        cost, _, h = session.run([loss, train_op,lm.final_h_],\n",
    "                       feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V=1000\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=100, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "# TF_SAVEDIR = \"tf_saved\"\n",
    "# checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "# trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-576b79a3ba51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnnlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildCoreGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildTrainGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mBuildCoreGraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((input_,))\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   3719\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m   \"\"\"\n\u001b[0;32m-> 3721\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3722\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3]."
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "# shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "# if not os.path.isdir(TF_SAVEDIR):\n",
    "#     os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "#         bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "        corpus = X_sample[0].split()\n",
    "        bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)\n",
    "        for i, (w,y) in enumerate(bi):\n",
    "            w1=w\n",
    "            \n",
    "        bi= (w1,y_sample[y])\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=learning_rate, train=True, \n",
    "                     verbose=False, tick_s=3600)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/legu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "corpus = nltk.corpus.brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vocabulary\n",
      "  Downloading Vocabulary-1.0.4.tar.gz\n",
      "Collecting requests==2.13.0 (from vocabulary)\n",
      "  Downloading requests-2.13.0-py2.py3-none-any.whl (584kB)\n",
      "\u001b[K    100% || 593kB 408kB/s \n",
      "\u001b[?25hCollecting mock==2.0.0 (from vocabulary)\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% || 61kB 1.9MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): funcsigs>=1; python_version < \"3.3\" in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.9 in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Collecting pbr>=0.11 (from mock==2.0.0->vocabulary)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% || 102kB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: vocabulary\n",
      "  Running setup.py bdist_wheel for vocabulary ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/legu/Library/Caches/pip/wheels/36/6c/c0/92bb20f79402d055c3bce3e89d9f2cce5d6937bc2aadc0fb45\n",
      "Successfully built vocabulary\n",
      "Installing collected packages: requests, pbr, mock, vocabulary\n",
      "  Found existing installation: requests 2.11.1\n",
      "    Uninstalling requests-2.11.1:\n",
      "      Successfully uninstalled requests-2.11.1\n",
      "Successfully installed mock-2.0.0 pbr-3.1.1 requests-2.13.0 vocabulary-1.0.4\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'Vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-385fd0cf2b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Train set vocabulary: %d words\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'Vocabulary'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import vocabulary\n",
    "# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\n",
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in utils.flatten(corpus))\n",
    "print \"Train set vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
