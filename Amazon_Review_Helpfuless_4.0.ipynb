{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Sklearn libraries.\n",
    "from sklearn import datasets, linear_model, ensemble, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "#deep learning library\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, tf_embed_viz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip gz file.\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "# Load JSON into dataframe.\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Used 42.877285s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = getDF('/home/legu/data/reviews_Clothing_Shoes_and_Jewelry_5.json.gz')\n",
    "end = time.time()\n",
    "print \"Time Used %fs\" %(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'reviewerID', u'asin', u'reviewerName', u'helpful', u'unixReviewTime',\n",
      "       u'reviewText', u'overall', u'reviewTime', u'summary'],\n",
      "      dtype='object')\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1KLRMWW2FWPL4</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer \"cameramom\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1297468800</td>\n",
       "      <td>This is a great tutu and at a really great pri...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>02 12, 2011</td>\n",
       "      <td>Great tutu-  not cheaply made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2G5TCU2WDFZ65</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1358553600</td>\n",
       "      <td>I bought this for my 4 yr old daughter for dan...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 19, 2013</td>\n",
       "      <td>Very Cute!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1RLQXYNCMWRWN</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Carola</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1357257600</td>\n",
       "      <td>What can I say... my daughters have it in oran...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 4, 2013</td>\n",
       "      <td>I have buy more than one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A8U3FAMSJVHS5</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Caromcg</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1398556800</td>\n",
       "      <td>We bought several tutus at once, and they are ...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>04 27, 2014</td>\n",
       "      <td>Adorable, Sturdy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3GEOILWLK86XM</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>CJ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1394841600</td>\n",
       "      <td>Thank you Halo Heaven great product for Little...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>Grammy's Angels Love it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                 reviewerName helpful  \\\n",
       "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
       "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
       "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
       "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
       "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
       "\n",
       "   unixReviewTime                                         reviewText  overall  \\\n",
       "0      1297468800  This is a great tutu and at a really great pri...   5.0000   \n",
       "1      1358553600  I bought this for my 4 yr old daughter for dan...   5.0000   \n",
       "2      1357257600  What can I say... my daughters have it in oran...   5.0000   \n",
       "3      1398556800  We bought several tutus at once, and they are ...   5.0000   \n",
       "4      1394841600  Thank you Halo Heaven great product for Little...   5.0000   \n",
       "\n",
       "    reviewTime                        summary  \n",
       "0  02 12, 2011  Great tutu-  not cheaply made  \n",
       "1  01 19, 2013                    Very Cute!!  \n",
       "2   01 4, 2013       I have buy more than one  \n",
       "3  04 27, 2014               Adorable, Sturdy  \n",
       "4  03 15, 2014        Grammy's Angels Love it  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.columns\n",
    "\n",
    "print df['reviewText'][0]\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring helpfulness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helpfulness = []\n",
    "total_votes = []\n",
    "for i in df['helpful']:\n",
    "    if i[1] == 0:\n",
    "        helpfulness.append(np.nan)\n",
    "        total_votes.append(np.nan)\n",
    "    else:\n",
    "        helpfulness.append(float(i[0])/i[1])\n",
    "        total_votes.append(i[1])\n",
    "        \n",
    "# Convert to numpy array.\n",
    "helpfulness = np.array(helpfulness)\n",
    "total_votes = np.array(total_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot boxplot.\n",
    "nonnan_helpfulness = helpfulness[~np.isnan(helpfulness)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove invalid and nan values.\n",
    "helpfulness_clean = np.delete(helpfulness, 30730)\n",
    "\n",
    "nonnan_helpfulness_clean = helpfulness_clean[~np.isnan(helpfulness_clean)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract the review text\n",
    "doc = np.array(df['reviewText'])\n",
    "\n",
    "# Filter down to reviews with helpfulness scores.\n",
    "# Remove item with invalid helpfulness score.\n",
    "doc_clean = np.delete(doc, 30730)\n",
    "nonnan_doc_clean = doc_clean[~np.isnan(helpfulness_clean)]\n",
    "y = np.reshape(nonnan_helpfulness_clean,(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep Learning Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Split the dataset into traing and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(nonnan_doc_clean )\n",
    "msk = np.random.rand(nonnan_doc_clean.shape[0]) <= 0.7\n",
    "\n",
    "X_train = nonnan_doc_clean[msk]\n",
    "X_test = nonnan_doc_clean[~msk]\n",
    "y_train = y[msk]\n",
    "y_test = y[~msk]\n",
    "X_sample = nonnan_doc_clean[0:10000]\n",
    "y_sample = y[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Full and well stitched.  This tutu is a beautiful purple color that looks just like the picture.  It looks just adorable on our little fairy.'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60922\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print (len(X_train))\n",
    "print (len(y_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Convert the word into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(docs):\n",
    "    tokens =[]\n",
    "    for i in docs:\n",
    "        lowers = i.lower()\n",
    "        #remove the punctuation using the character deletion step of translate\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token = nltk.word_tokenize(no_punctuation)\n",
    "        tokens.extend(token)\n",
    "    return tokens\n",
    "\n",
    "tokens = get_tokens(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic, rev_dict = build_dataset(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['limited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ability'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_dict[2102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sample_ids = []\n",
    "for i in X_sample:\n",
    "    lowers = i.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token = nltk.word_tokenize(no_punctuation)\n",
    "    j=[dic[w] for w in token]\n",
    "    X_sample_ids.append(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For what I paid for two tutus is unbeatable anywhere!  I ordered a pink and turquios and they are vibrant and beautiful! The tutu is very full! Princess style! Not cheaply made! Not cheap materia! Obviously someone made these with love and care! I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two ! Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!\n",
      "[8, 83, 1, 609, 8, 130, 10316, 6, 6286, 996, 1, 96, 3, 686, 2, 15274, 2, 12, 13, 2554, 2, 355, 0, 9136, 6, 26, 404, 3355, 224, 19, 1646, 87, 19, 285, 24819, 1211, 502, 87, 17, 20, 78, 2, 540, 1, 609, 282, 52, 468, 1172, 8, 3, 9136, 1, 2, 1, 117, 4277, 7, 11, 1648, 8, 4646, 4, 0, 591, 7, 767, 24030, 3689, 17126, 519, 6, 130, 424, 29, 8633, 2, 10, 1584, 33, 29, 36, 110, 2, 41, 22416, 27, 349, 4, 1772, 250, 504, 3146, 195, 8, 81, 57]\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "print (X_sample[0])\n",
    "print (X_sample_ids[0])\n",
    "print (y_sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.build tensor flow model - training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(np.column_stack([X_sample_ids,y_sample]), \n",
    "                               columns=['list_words', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 83, 1, 609, 8, 130, 10316, 6, 6286, 996, 1...</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[404, 2, 48, 2067, 10, 9136, 6, 3, 355, 1280, ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3668, 1243, 2, 5, 6, 401, 23, 1110, 0, 76, 48...</td>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 971, 1, 636, 10, 11, 1867, 7963, 3, 970, 8...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[78, 10, 584, 304, 24, 48, 143, 697, 560, 2659...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          list_words  label\n",
       "0  [8, 83, 1, 609, 8, 130, 10316, 6, 6286, 996, 1... 0.8750\n",
       "1  [404, 2, 48, 2067, 10, 9136, 6, 3, 355, 1280, ... 1.0000\n",
       "2  [3668, 1243, 2, 5, 6, 401, 23, 1110, 0, 76, 48... 0.5000\n",
       "3  [0, 971, 1, 636, 10, 11, 1867, 7963, 3, 970, 8... 1.0000\n",
       "4  [78, 10, 584, 304, 24, 48, 143, 697, 560, 2659... 0.0000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "        return res['list_words'], res['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [38, 141, 31, 1337, 1098, 104, 4383, 4824, 193...\n",
      "1    [12, 423, 26, 928, 43, 84, 2, 1437, 236, 3, 10...\n",
      "2    [19, 82, 21, 53, 187, 64, 3, 496, 15, 17, 13, ...\n",
      "Name: list_words, dtype: object\n",
      "0   1.0000\n",
      "1   1.0000\n",
      "2   0.0000\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataIterator(train_df)\n",
    "d = data.next_batch(3)\n",
    "print(d[0])\n",
    "print(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n,max_len):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "#             self.max_len = max_len\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        maxlen = max_len\n",
    "        x = np.zeros([n, maxlen], dtype=np.int32)\n",
    "        for i, x_i in enumerate(x):\n",
    "            l=len(res['list_words'].values[i]) ##list length\n",
    "            if l>maxlen: \n",
    "                x_i[:maxlen] = res['list_words'].values[i][:max_len]\n",
    "            else:\n",
    "                x_i[:l] = res['list_words'].values[i][:l]\n",
    "\n",
    "        return x, res['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   1.0000\n",
      "1   1.0000\n",
      "2   1.0000\n",
      "3   0.5000\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_pad = PaddedDataIterator(train_df)\n",
    "d = data_pad.next_batch(4,100)\n",
    "print(d[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(4,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1341,    10,   387,     6,    26,   673,     1,    65,  3071,\n",
       "            2,     3,    29,  6988,  1590,     1,    65,   448,    29,\n",
       "         1559,     2,  1071,     9,   195,     1,   510,     5,     0,\n",
       "           76,   221,     6,     0,  1590,   494,     6,   121,     4,\n",
       "          120,     8,    45,    72,  2201,     5,   232,    45,   117,\n",
       "           38,     1,    67,    19,    31,     5,    71,     5,   188,\n",
       "           36,     0,  1320,    24,    34,    21,    13,   546,     2,\n",
       "           18,  1901, 20903,    10,    41,    69,    38,    16,    21,\n",
       "           72,  2201,     5,   315,    36,   124,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [   25,     3,    77,   440,    20,    10,   432,   106,     7,\n",
       "           39,     5,     6,   156, 30112,     1,    25,   125,     8,\n",
       "            3,   329,   432,   369,     1,   204,     7,    63,     3,\n",
       "           29,  9368,    63,    68,   776,    79,   153,  1069,   953,\n",
       "           10,   432,   331,   156,    43,    48,    47,     1,   280,\n",
       "            5,    39,     1,   140,   137,     6, 18241,   224,  1609,\n",
       "        11945,   104,   432,     2,   279,    64,     5,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [   40,     3,   107,   518,     8,     3,    90,    30,  1420,\n",
       "         1689,   789,   297,    15,  1341,   210,   560,   347, 26516,\n",
       "            7, 27160,   648,     7,   305,     9,   371,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0],\n",
       "       [    1,    65,    24,   220,    20,    17,    12,   117,    38,\n",
       "            2,    26,   419,     0,   313,   838,    25,   567,     1,\n",
       "           65, 16179,     2,     1,    96,  5163,    80,    36,   230,\n",
       "            0,   985,   201,     6,    26,    50,   606,     4,    72,\n",
       "          566,     0,   213,     6,    26,   661,    19,    49,   943,\n",
       "           15,    19, 25981,   275,   397,     5,    79,     6,    19,\n",
       "         4991,    28,    72,  3869,  1374,    67,     1,   874,   111,\n",
       "         4062,    12,    13,    24,    50,     2,     1,   505,     4,\n",
       "           58,   160,    44,   424,    54,     7,    17,     1,    65,\n",
       "           24,   922,    20,    17,     1,    37,    63,   207,    44,\n",
       "          270,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0]], dtype=int32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  if __name__ == '__main__':\n",
      "/home/legu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/legu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6666666666666666, 0.33333333333333337],\n",
       "       [0.0, 1.0],\n",
       "       [1.0, 0.0],\n",
       "       [1.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.reshape([-1,1])\n",
    "1-batch_y.reshape([-1,1])\n",
    "\n",
    "np.concatenate((batch_y.reshape([-1,1]), 1-batch_y.reshape([-1,1])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/legu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:89: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100, Minibatch Loss= 0.943899, Training Accuracy= 0.17000\n",
      "Iter 200, Minibatch Loss= 0.904176, Training Accuracy= 0.24000\n",
      "Iter 300, Minibatch Loss= 0.912557, Training Accuracy= 0.22000\n",
      "Iter 400, Minibatch Loss= 0.906996, Training Accuracy= 0.27000\n",
      "Iter 500, Minibatch Loss= 0.899713, Training Accuracy= 0.21000\n",
      "Iter 600, Minibatch Loss= 0.901354, Training Accuracy= 0.24000\n",
      "Iter 700, Minibatch Loss= 0.884398, Training Accuracy= 0.28000\n",
      "Iter 800, Minibatch Loss= 0.946870, Training Accuracy= 0.15000\n",
      "Iter 900, Minibatch Loss= 0.893708, Training Accuracy= 0.23000\n",
      "Iter 1000, Minibatch Loss= 0.907581, Training Accuracy= 0.20000\n",
      "Iter 1100, Minibatch Loss= 0.851186, Training Accuracy= 0.23000\n",
      "Iter 1200, Minibatch Loss= 0.906922, Training Accuracy= 0.20000\n",
      "Iter 1300, Minibatch Loss= 0.896135, Training Accuracy= 0.21000\n",
      "Iter 1400, Minibatch Loss= 0.907591, Training Accuracy= 0.20000\n",
      "Iter 1500, Minibatch Loss= 0.875335, Training Accuracy= 0.22000\n",
      "Iter 1600, Minibatch Loss= 0.863656, Training Accuracy= 0.26000\n",
      "Iter 1700, Minibatch Loss= 0.887579, Training Accuracy= 0.26000\n",
      "Iter 1800, Minibatch Loss= 0.855921, Training Accuracy= 0.28000\n",
      "Iter 1900, Minibatch Loss= 0.913348, Training Accuracy= 0.20000\n",
      "Iter 2000, Minibatch Loss= 0.874614, Training Accuracy= 0.20000\n",
      "Iter 2100, Minibatch Loss= 0.894239, Training Accuracy= 0.19000\n",
      "Iter 2200, Minibatch Loss= 0.859209, Training Accuracy= 0.24000\n",
      "Iter 2300, Minibatch Loss= 0.847184, Training Accuracy= 0.25000\n",
      "Iter 2400, Minibatch Loss= 0.865572, Training Accuracy= 0.28000\n",
      "Iter 2500, Minibatch Loss= 0.827728, Training Accuracy= 0.31000\n",
      "Iter 2600, Minibatch Loss= 0.870537, Training Accuracy= 0.23000\n",
      "Iter 2700, Minibatch Loss= 0.851020, Training Accuracy= 0.21000\n",
      "Iter 2800, Minibatch Loss= 0.841387, Training Accuracy= 0.27000\n",
      "Iter 2900, Minibatch Loss= 0.836051, Training Accuracy= 0.25000\n",
      "Iter 3000, Minibatch Loss= 0.843593, Training Accuracy= 0.29000\n",
      "Iter 3100, Minibatch Loss= 0.896719, Training Accuracy= 0.19000\n",
      "Iter 3200, Minibatch Loss= 0.877029, Training Accuracy= 0.24000\n",
      "Iter 3300, Minibatch Loss= 0.852124, Training Accuracy= 0.24000\n",
      "Iter 3400, Minibatch Loss= 0.809688, Training Accuracy= 0.30000\n",
      "Iter 3500, Minibatch Loss= 0.854719, Training Accuracy= 0.24000\n",
      "Iter 3600, Minibatch Loss= 0.868445, Training Accuracy= 0.22000\n",
      "Iter 3700, Minibatch Loss= 0.827808, Training Accuracy= 0.28000\n",
      "Iter 3800, Minibatch Loss= 0.822698, Training Accuracy= 0.33000\n",
      "Iter 3900, Minibatch Loss= 0.840134, Training Accuracy= 0.23000\n",
      "Iter 4000, Minibatch Loss= 0.840142, Training Accuracy= 0.27000\n",
      "Iter 4100, Minibatch Loss= 0.865387, Training Accuracy= 0.19000\n",
      "Iter 4200, Minibatch Loss= 0.825262, Training Accuracy= 0.24000\n",
      "Iter 4300, Minibatch Loss= 0.849050, Training Accuracy= 0.26000\n",
      "Iter 4400, Minibatch Loss= 0.851981, Training Accuracy= 0.27000\n",
      "Iter 4500, Minibatch Loss= 0.757177, Training Accuracy= 0.50000\n",
      "Iter 4600, Minibatch Loss= 0.819478, Training Accuracy= 0.30000\n",
      "Iter 4700, Minibatch Loss= 0.826915, Training Accuracy= 0.32000\n",
      "Iter 4800, Minibatch Loss= 0.824870, Training Accuracy= 0.27000\n",
      "Iter 4900, Minibatch Loss= 0.796269, Training Accuracy= 0.37000\n",
      "Iter 5000, Minibatch Loss= 0.798388, Training Accuracy= 0.33000\n",
      "Iter 5100, Minibatch Loss= 0.821578, Training Accuracy= 0.32000\n",
      "Iter 5200, Minibatch Loss= 0.827913, Training Accuracy= 0.30000\n",
      "Iter 5300, Minibatch Loss= 0.804844, Training Accuracy= 0.38000\n",
      "Iter 5400, Minibatch Loss= 0.792176, Training Accuracy= 0.40000\n",
      "Iter 5500, Minibatch Loss= 0.809863, Training Accuracy= 0.35000\n",
      "Iter 5600, Minibatch Loss= 0.796268, Training Accuracy= 0.39000\n",
      "Iter 5700, Minibatch Loss= 0.781362, Training Accuracy= 0.44000\n",
      "Iter 5800, Minibatch Loss= 0.783875, Training Accuracy= 0.36000\n",
      "Iter 5900, Minibatch Loss= 0.753197, Training Accuracy= 0.48000\n",
      "Iter 6000, Minibatch Loss= 0.791590, Training Accuracy= 0.40000\n",
      "Iter 6100, Minibatch Loss= 0.795970, Training Accuracy= 0.42000\n",
      "Iter 6200, Minibatch Loss= 0.785598, Training Accuracy= 0.41000\n",
      "Iter 6300, Minibatch Loss= 0.749991, Training Accuracy= 0.52000\n",
      "Iter 6400, Minibatch Loss= 0.801056, Training Accuracy= 0.38000\n",
      "Iter 6500, Minibatch Loss= 0.737436, Training Accuracy= 0.52000\n",
      "Iter 6600, Minibatch Loss= 0.799840, Training Accuracy= 0.42000\n",
      "Iter 6700, Minibatch Loss= 0.749019, Training Accuracy= 0.51000\n",
      "Iter 6800, Minibatch Loss= 0.790864, Training Accuracy= 0.42000\n",
      "Iter 6900, Minibatch Loss= 0.774523, Training Accuracy= 0.44000\n",
      "Iter 7000, Minibatch Loss= 0.755017, Training Accuracy= 0.52000\n",
      "Iter 7100, Minibatch Loss= 0.699194, Training Accuracy= 0.55000\n",
      "Iter 7200, Minibatch Loss= 0.751504, Training Accuracy= 0.48000\n",
      "Iter 7300, Minibatch Loss= 0.766456, Training Accuracy= 0.47000\n",
      "Iter 7400, Minibatch Loss= 0.771636, Training Accuracy= 0.48000\n",
      "Iter 7500, Minibatch Loss= 0.769641, Training Accuracy= 0.47000\n",
      "Iter 7600, Minibatch Loss= 0.718589, Training Accuracy= 0.54000\n",
      "Iter 7700, Minibatch Loss= 0.722794, Training Accuracy= 0.51000\n",
      "Iter 7800, Minibatch Loss= 0.750976, Training Accuracy= 0.48000\n",
      "Iter 7900, Minibatch Loss= 0.774792, Training Accuracy= 0.48000\n",
      "Iter 8000, Minibatch Loss= 0.758251, Training Accuracy= 0.49000\n",
      "Iter 8100, Minibatch Loss= 0.695590, Training Accuracy= 0.60000\n",
      "Iter 8200, Minibatch Loss= 0.731848, Training Accuracy= 0.53000\n",
      "Iter 8300, Minibatch Loss= 0.745971, Training Accuracy= 0.53000\n",
      "Iter 8400, Minibatch Loss= 0.762368, Training Accuracy= 0.51000\n",
      "Iter 8500, Minibatch Loss= 0.730161, Training Accuracy= 0.54000\n",
      "Iter 8600, Minibatch Loss= 0.698612, Training Accuracy= 0.56000\n",
      "Iter 8700, Minibatch Loss= 0.731433, Training Accuracy= 0.56000\n",
      "Iter 8800, Minibatch Loss= 0.737221, Training Accuracy= 0.51000\n",
      "Iter 8900, Minibatch Loss= 0.690746, Training Accuracy= 0.62000\n",
      "Iter 9000, Minibatch Loss= 0.745923, Training Accuracy= 0.51000\n",
      "Iter 9100, Minibatch Loss= 0.759447, Training Accuracy= 0.51000\n",
      "Iter 9200, Minibatch Loss= 0.694777, Training Accuracy= 0.60000\n",
      "Iter 9300, Minibatch Loss= 0.774381, Training Accuracy= 0.51000\n",
      "Iter 9400, Minibatch Loss= 0.683467, Training Accuracy= 0.55000\n",
      "Iter 9500, Minibatch Loss= 0.705410, Training Accuracy= 0.60000\n",
      "Iter 9600, Minibatch Loss= 0.724001, Training Accuracy= 0.53000\n",
      "Iter 9700, Minibatch Loss= 0.762806, Training Accuracy= 0.50000\n",
      "Iter 9800, Minibatch Loss= 0.667686, Training Accuracy= 0.62000\n",
      "Iter 9900, Minibatch Loss= 0.684024, Training Accuracy= 0.58000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "\n",
    "\n",
    "'''\n",
    "To classify images using a recurrent neural network, we consider every image\n",
    "row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
    "handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 10000\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 10 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 10 # timesteps\n",
    "n_hidden = 10 # hidden layer num of features\n",
    "n_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    with tf.variable_scope(\"first_lstm29\"):\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "  \n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.nn.softmax(tf.matmul(outputs[-1], weights['out']) + biases['out'])\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y_1 = PaddedDataIterator(train_df).next_batch(batch_size,100)\n",
    "        batch_y = np.concatenate((batch_y_1.reshape([-1,1]), 1-batch_y_1.reshape([-1,1])), axis=1)\n",
    "\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "#             print( sess.run(pred, feed_dict={x: batch_x, y: batch_y}))\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "#     test_len = 128\n",
    "#     test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "#     test_label = mnist.test.labels[:test_len]\n",
    "#     print(\"Testing Accuracy:\", \\\n",
    "#         sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,  176,   34,   33,   55,  473,    5,  371,   68,  111,    4,\n",
       "        2523,  133,    5,   34,  371,    3,  140,  539,    2,   55,    1,\n",
       "        2409,   22,    1,   64,  210,   20,  237,    5,  175,  335,   21,\n",
       "           5,  193,  271,    5, 1097,  326,   53,    1,  216,   57,    9,\n",
       "        1349,    1,   17,   10,  150,   15,  266,    4,  213,   31, 2480,\n",
       "           9,  849,    1,   17,   15, 2183, 1508,  426,   11,   15,  352,\n",
       "          13, 2397,  432,    4,    3,  179, 1624,   40,    0, 2087,  663,\n",
       "           4,  891,  819,    2,  215,  192,   11,  512,    0,  266,  822,\n",
       "           7,    3,   62,  207,   23,    1,   43,    2,  612,    5,  125,\n",
       "          23],\n",
       "       [   1,  182,   10,   91,    3,   62,  100,    3,  160,  329,  136,\n",
       "          12,    1,   78,  451,   13,    5,    1,  263,    2,  968,    5,\n",
       "           1,  124,    5,   16, 1273,    4, 1381,   36,   43,  131, 2129,\n",
       "        1394,    0, 1723,    4,   17,  101,  692,    0, 1882,   11,    0,\n",
       "        1361, 2415, 2062, 1912,   16,    0,  732,    5,  839,   12,    0,\n",
       "         695,    7,   20, 1188,   23,    6,  220,  968,    5,   21,    3,\n",
       "         110,  423,   99,  148,  451,   13, 2383, 2179,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 182,   10,  242,    8,   15,  194,    2,  313,  123,  601,  563,\n",
       "          11,    4,    5,  389,   14,    0,  538,    9,    0,  173,  563,\n",
       "         111,    0, 1992,  551,  882,  125,  638,    1,   72,    5,   41,\n",
       "          33,  251,   22,    6,   60,  124,  327,  421,    0,  242,   14,\n",
       "          22,    6,  112,    2,  124,  563,   11,   92,   40,   15,  194,\n",
       "          47,    6,   41,   17,    2, 2361,   16,   24,  165,  551,   26,\n",
       "         954,  429,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  10,   39, 1157, 1910,    4,  265, 2389,  125,    2,    3, 1613,\n",
       "         234, 1267,    7, 1455,    2,   35,    3,  179, 1495,   52,  169,\n",
       "        2577,   11, 2478, 2181, 1830,    4,   79,    3, 1563, 1538,   26,\n",
       "        1244,   27,  159,  440,  159,   14, 1975,   22,    6,  112,    3,\n",
       "          62,   50,  817,  130,   13,    0, 2066,  146, 1874,  196,   12,\n",
       "          63,  905,    0,  519,    7,    3,  440, 1450,   99,   51, 1502,\n",
       "         629,    9,    3, 1015,   39,   13,    3,  519,  196,    0,  449,\n",
       "           7,  553, 2402,  168,    0,  151, 1632,  114, 2212,    4, 2238,\n",
       "         198,    0,   65,   96,  410,    9,  121,    5,  212,  435,   14,\n",
       "          27],\n",
       "       [   1,  182,   10,  103,    2,  406,   96,  427,  171,  252,  370,\n",
       "          97,  461,   16,  439,  128,   34,  114,  472,    4,    0,  103,\n",
       "          18,    3,  493, 1120,   15,  427,  104,   20,   17,   31,  609,\n",
       "          23,   10, 1319,  103,  104,   27,  338,   68,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 478, 1983,   18,   42,    1, 2514,  365,    4,   45,  238, 2338,\n",
       "          78,  589,   13,    0, 2435,  155, 2322,  194,  566,    5,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  74,  238,    1,  114, 1981,   42,    1,  328,    4,  492,    5,\n",
       "          11,  161,  784,    1, 1291,    4,   48,    2,   15,  767,    4,\n",
       "         533,   21,   68,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 110,   10,   16,  333,  546,  439, 1131,   52, 1845,   97,   24,\n",
       "         663,   26,  277,    8,  195,   26, 1115,    1,  745,  205,    5,\n",
       "         389,  209,  213,   24,  549,   43,   15,  195,  405,  860,    4,\n",
       "         426,   11,    5,  252,  225,   97,    0,  461,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [   2,   15,   80,    9, 1787,   92,   19,   49,   96, 2589,  142,\n",
       "         215,    0,  579,   52, 1646,    3,  531, 2544,  404,  120,    7,\n",
       "           0,   39,  233,    5,    7,   20,   32,  391,    5,  249,   79,\n",
       "         500,   27,  111,   57,    9,   32,  190,  845,  642,    1,  165,\n",
       "         301,  211,    4,   79,    0, 2473,  500,  233,  422,  559,   37,\n",
       "           0, 2317,    9,    3,  815,   14,   10,   31,  534,    5,    2,\n",
       "           3,  381,  146,   77,  239,   33,  878,    2, 1862,   76,  380,\n",
       "           8, 1081,    5,  273, 1316,    7,    0,  465,   12,  598,    0,\n",
       "        1483,    9,  138,  503,  880,    2,  582, 1431,  812, 1533,  450,\n",
       "          10],\n",
       "       [  10,   18,    3,   32,  446,  426,   12,    1,  110,   11, 2123,\n",
       "         243,    7, 1373, 2559,   23,    5,  193,   20,  435,   97,    3,\n",
       "         549,   14,  268,    2,  157,    3, 1758,   26,    3, 1498,    8,\n",
       "         575, 2345,  591,  877,  574,  244, 2572,   30,  190,  313, 1213,\n",
       "        1776, 2340,   13, 1249,   30,  313, 1948, 1077,   26,  917,   22,\n",
       "           6, 1865,  207,  266,  103,   12,   30,   33, 1310,   59, 1795,\n",
       "           3,  681,  426,   11, 2225,  649,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]], dtype=int32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)\n",
    "batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "\n",
    "x = tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_5:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:1' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:2' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:3' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:4' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:5' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:6' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:7' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:8' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:9' shape=(?, 10) dtype=float32>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define variables \n",
    "n_hidden = 10 \n",
    "with tf.variable_scope(\"softmax\"):\n",
    "    W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "    b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "\n",
    "with tf.variable_scope(\"recurrent\"):\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    initial_h_ = self.cell_.zero_state(self.batch_size_,dtype=tf.float32)\n",
    "    outputs, self.final_h_ = tf.nn.dynamic_rnn(self.cell_, inputs=self.x_,initial_state=self.initial_h_, sequence_length=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for _ in xrange(num_layers):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(H, forget_bias=0.0)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "        cells.append(cell)\n",
    "    return tf.contrib.rnn.MultiRNNCell(cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##have to fix here!!!\n",
    "def RNN(x):\n",
    "    n_hidden = 10 \n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "        b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "    # reshape to [1, n_input]\n",
    "    n_input = len(x)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "#     print x\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "#     with tf.variable_scope('cell_def'):\n",
    "#         lstm_cell = MakeFancyRNNCell(H=n_hidden,keep_prob=1)\n",
    "    lstm_cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(n_hidden, forget_bias=0.0)\n",
    "#     print x\n",
    "    # generate prediction\n",
    "\n",
    "    \n",
    "#     with tf.variable_scope('rnn_def',reuse=True):\n",
    "    initial_h_ = lstm_cell.zero_state(1,dtype=tf.int32)\n",
    "    outputs,final_h_ = tf.contrib.rnn.static_rnn(lstm_cell, x,dtype=tf.int32)\n",
    "            \n",
    "            \n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], W_out_) + b_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'static_rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-320bf1dbf5de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sample_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-c600588cd114>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     with tf.variable_scope('rnn_def',reuse=True):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'static_rnn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "learning_rate =0.1\n",
    "pred = RNN(X_sample_ids[0])\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_sample[0]))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, loss, onehot_pred = session.run([optimizer, cost, pred], \n",
    "                                        feed_dict={x: symbols_in_keys, y: symbols_out_onehot})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60843,)\n",
      "(26177,)\n",
      "(60843, 1)\n",
      "(26177, 1)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sample = nonnan_doc_clean[0:100]\n",
    "y_sample = y[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "print len(X_sample[0].split())\n",
    "print X_sample[0]\n",
    "print y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.__getattr__('brown').words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = X_sample[0].split()\n",
    "bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['For' 'what' 'I' 'paid' 'for' 'two' 'tutus' 'is' 'unbeatable' 'anywhere!'\n",
      "  'I' 'ordered' 'a' 'pink' 'and' 'turquios' 'and' 'they' 'are' 'vibrant'\n",
      "  'and' 'beautiful!' 'The' 'tutu' 'is' 'very' 'full!' 'Princess' 'style!'\n",
      "  'Not' 'cheaply' 'made!' 'Not' 'cheap' 'materia!' 'Obviously' 'someone'\n",
      "  'made' 'these' 'with' 'love' 'and' 'care!' 'I' 'paid' 'less' 'than' '7'\n",
      "  'bucks' 'for' 'a' 'tutu' 'I' 'and' 'I' 'feel' 'proud' 'of' 'my' 'self'\n",
      "  'for' 'researching' 'to' 'the' 'point' 'of' 'finding' 'gold!Recommend'\n",
      "  '2-6' 'years!My' 'daughter' 'is' 'two' '!' 'Wears' 'size' '4t' 'and'\n",
      "  'this' 'skirt' '(' 'one' 'size' ')' 'fit' 'perfect' 'and' 'will'\n",
      "  'probaly' 'be' 'able' 'to' 'accommodate' 'her' 'quickly' 'growing'\n",
      "  'waist' 'for' 'some']]\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "for i, (w,y) in enumerate(bi):\n",
    "    print w\n",
    "    print y_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "# reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.learning_rate_:learning_rate,\n",
    "                 lm.initial_h_ : h}\n",
    "            \n",
    "        cost, _, h = session.run([loss, train_op,lm.final_h_],\n",
    "                       feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V=1000\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=100, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "# TF_SAVEDIR = \"tf_saved\"\n",
    "# checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "# trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-576b79a3ba51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnnlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildCoreGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildTrainGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mBuildCoreGraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((input_,))\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   3719\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m   \"\"\"\n\u001b[0;32m-> 3721\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3722\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3]."
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "# shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "# if not os.path.isdir(TF_SAVEDIR):\n",
    "#     os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "#         bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "        corpus = X_sample[0].split()\n",
    "        bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)\n",
    "        for i, (w,y) in enumerate(bi):\n",
    "            w1=w\n",
    "            \n",
    "        bi= (w1,y_sample[y])\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=learning_rate, train=True, \n",
    "                     verbose=False, tick_s=3600)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/legu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "corpus = nltk.corpus.brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vocabulary\n",
      "  Downloading Vocabulary-1.0.4.tar.gz\n",
      "Collecting requests==2.13.0 (from vocabulary)\n",
      "  Downloading requests-2.13.0-py2.py3-none-any.whl (584kB)\n",
      "\u001b[K    100% || 593kB 408kB/s \n",
      "\u001b[?25hCollecting mock==2.0.0 (from vocabulary)\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% || 61kB 1.9MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): funcsigs>=1; python_version < \"3.3\" in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.9 in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Collecting pbr>=0.11 (from mock==2.0.0->vocabulary)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% || 102kB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: vocabulary\n",
      "  Running setup.py bdist_wheel for vocabulary ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/legu/Library/Caches/pip/wheels/36/6c/c0/92bb20f79402d055c3bce3e89d9f2cce5d6937bc2aadc0fb45\n",
      "Successfully built vocabulary\n",
      "Installing collected packages: requests, pbr, mock, vocabulary\n",
      "  Found existing installation: requests 2.11.1\n",
      "    Uninstalling requests-2.11.1:\n",
      "      Successfully uninstalled requests-2.11.1\n",
      "Successfully installed mock-2.0.0 pbr-3.1.1 requests-2.13.0 vocabulary-1.0.4\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'Vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-385fd0cf2b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Train set vocabulary: %d words\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'Vocabulary'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import vocabulary\n",
    "# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\n",
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in utils.flatten(corpus))\n",
    "print \"Train set vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
