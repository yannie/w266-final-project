{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "# Sklearn libraries.\n",
    "from sklearn import datasets, linear_model, ensemble, metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "#deep learning library\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, tf_embed_viz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip gz file.\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "# Load JSON into dataframe.\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Used 42.613845s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = getDF('reviews_Clothing_Shoes_and_Jewelry_5.json.gz')\n",
    "end = time.time()\n",
    "print \"Time Used %fs\" %(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'reviewerID', u'asin', u'reviewerName', u'helpful', u'unixReviewTime',\n",
      "       u'reviewText', u'overall', u'reviewTime', u'summary'],\n",
      "      dtype='object')\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1KLRMWW2FWPL4</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer \"cameramom\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1297468800</td>\n",
       "      <td>This is a great tutu and at a really great pri...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>02 12, 2011</td>\n",
       "      <td>Great tutu-  not cheaply made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2G5TCU2WDFZ65</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1358553600</td>\n",
       "      <td>I bought this for my 4 yr old daughter for dan...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 19, 2013</td>\n",
       "      <td>Very Cute!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1RLQXYNCMWRWN</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Carola</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1357257600</td>\n",
       "      <td>What can I say... my daughters have it in oran...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>01 4, 2013</td>\n",
       "      <td>I have buy more than one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A8U3FAMSJVHS5</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>Caromcg</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1398556800</td>\n",
       "      <td>We bought several tutus at once, and they are ...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>04 27, 2014</td>\n",
       "      <td>Adorable, Sturdy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3GEOILWLK86XM</td>\n",
       "      <td>0000031887</td>\n",
       "      <td>CJ</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1394841600</td>\n",
       "      <td>Thank you Halo Heaven great product for Little...</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>Grammy's Angels Love it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin                 reviewerName helpful  \\\n",
       "0  A1KLRMWW2FWPL4  0000031887  Amazon Customer \"cameramom\"  [0, 0]   \n",
       "1  A2G5TCU2WDFZ65  0000031887              Amazon Customer  [0, 0]   \n",
       "2  A1RLQXYNCMWRWN  0000031887                       Carola  [0, 0]   \n",
       "3   A8U3FAMSJVHS5  0000031887                      Caromcg  [0, 0]   \n",
       "4  A3GEOILWLK86XM  0000031887                           CJ  [0, 0]   \n",
       "\n",
       "   unixReviewTime                                         reviewText  overall  \\\n",
       "0      1297468800  This is a great tutu and at a really great pri...   5.0000   \n",
       "1      1358553600  I bought this for my 4 yr old daughter for dan...   5.0000   \n",
       "2      1357257600  What can I say... my daughters have it in oran...   5.0000   \n",
       "3      1398556800  We bought several tutus at once, and they are ...   5.0000   \n",
       "4      1394841600  Thank you Halo Heaven great product for Little...   5.0000   \n",
       "\n",
       "    reviewTime                        summary  \n",
       "0  02 12, 2011  Great tutu-  not cheaply made  \n",
       "1  01 19, 2013                    Very Cute!!  \n",
       "2   01 4, 2013       I have buy more than one  \n",
       "3  04 27, 2014               Adorable, Sturdy  \n",
       "4  03 15, 2014        Grammy's Angels Love it  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.columns\n",
    "\n",
    "print df['reviewText'][0]\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring helpfulness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "helpfulness = []\n",
    "total_votes = []\n",
    "for i in df['helpful']:\n",
    "    if i[1] == 0:\n",
    "        helpfulness.append(np.nan)\n",
    "        total_votes.append(np.nan)\n",
    "    else:\n",
    "        helpfulness.append(float(i[0])/i[1])\n",
    "        total_votes.append(i[1])\n",
    "        \n",
    "# Convert to numpy array.\n",
    "helpfulness = np.array(helpfulness)\n",
    "total_votes = np.array(total_votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot boxplot.\n",
    "nonnan_helpfulness = helpfulness[~np.isnan(helpfulness)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove invalid and nan values.\n",
    "helpfulness_clean = np.delete(helpfulness, 30730)\n",
    "\n",
    "nonnan_helpfulness_clean = helpfulness_clean[~np.isnan(helpfulness_clean)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#extract the review text\n",
    "doc = np.array(df['reviewText'])\n",
    "\n",
    "# Filter down to reviews with helpfulness scores.\n",
    "# Remove item with invalid helpfulness score.\n",
    "doc_clean = np.delete(doc, 30730)\n",
    "nonnan_doc_clean = doc_clean[~np.isnan(helpfulness_clean)]\n",
    "y = np.reshape(nonnan_helpfulness_clean,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_norm = np.rint(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Deep Learning Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Split the dataset into traing and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(nonnan_doc_clean )\n",
    "msk = np.random.rand(nonnan_doc_clean.shape[0]) <= 0.7\n",
    "\n",
    "X_train = nonnan_doc_clean[msk]\n",
    "X_test = nonnan_doc_clean[~msk]\n",
    "y_train = y_norm[msk]\n",
    "y_test = y_norm[~msk]\n",
    "X_sample = nonnan_doc_clean[0:60000]\n",
    "y_sample = y_norm[0:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The minute I saw this my heart skipped a beat. What a nice case to sort my collection. I love all the compartments and the layout too. You will love this color too. If this is a gift do not hesitate. It will please even the most fussy, as I am. The price is fair if you check around as I did. Seems well made too. It is suitable for an adult though. Not meant as a toy. This is for serious jewelry lover's. Buy it.....\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61032\n",
      "25988\n",
      "25988\n"
     ]
    }
   ],
   "source": [
    "print (len(X_train))\n",
    "print (len(X_test))\n",
    "print (len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Convert the review from list of words to list of word dictionary index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens(docs):\n",
    "    tokens =[]\n",
    "    for i in docs:\n",
    "        lowers = i.lower()\n",
    "        #remove the punctuation using the character deletion step of translate\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token = nltk.word_tokenize(no_punctuation)\n",
    "        tokens.extend(token)\n",
    "    return tokens\n",
    "\n",
    "tokens = get_tokens(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vocab size.\n",
    "V = 10000\n",
    "def build_dataset(words):\n",
    "    # Leave the last index for all other words.\n",
    "    count = collections.Counter(words).most_common(V-1)\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic, rev_dict = build_dataset(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9998"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic['full']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_dict[1461]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build sample data\n",
    "X_sample_ids = []\n",
    "for i in X_sample:\n",
    "    lowers = i.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token = nltk.word_tokenize(no_punctuation)\n",
    "    j = []\n",
    "    for w in token:\n",
    "        if w in dic:\n",
    "            j.append(dic[w])\n",
    "        # Set token_id to vocab size - 1 if word not in top words\n",
    "        # since this is zero indexed.\n",
    "        else:\n",
    "            j.append(V-1)\n",
    "    X_sample_ids.append(j)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build test data, note if word not in the dictionary, ignore\n",
    "X_test_ids = []\n",
    "for i in X_test:\n",
    "    lowers = i.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token = nltk.word_tokenize(no_punctuation)\n",
    "    j = []\n",
    "    for w in token:\n",
    "        if w in dic:\n",
    "            j.append(dic[w])\n",
    "        # Set token_id to vocab size - 1 if word not in top words\n",
    "        # since this is zero indexed.\n",
    "        else:\n",
    "            j.append(V-1)\n",
    "    X_test_ids.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For what I paid for two tutus is unbeatable anywhere!  I ordered a pink and turquios and they are vibrant and beautiful! The tutu is very full! Princess style! Not cheaply made! Not cheap materia! Obviously someone made these with love and care! I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two ! Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!\n",
      "[8, 86, 1, 611, 8, 134, 9999, 6, 5996, 1122, 1, 84, 3, 567, 2, 9999, 2, 12, 13, 1811, 2, 255, 0, 6132, 6, 26, 440, 3268, 194, 19, 1510, 90, 19, 259, 9999, 1183, 479, 90, 17, 20, 55, 2, 623, 1, 611, 326, 52, 415, 1225, 8, 3, 6132, 1, 2, 1, 115, 3151, 7, 11, 1969, 8, 5697, 4, 0, 580, 7, 813, 9999, 3345, 9999, 414, 6, 134, 460, 29, 6679, 2, 10, 615, 37, 29, 32, 96, 2, 42, 9999, 28, 338, 4, 1768, 206, 473, 2884, 225, 8, 87, 76]\n",
      "[ 1.]\n",
      "For what I paid for two tutus is unbeatable anywhere!  I ordered a pink and turquios and they are vibrant and beautiful! The tutu is very full! Princess style! Not cheaply made! Not cheap materia! Obviously someone made these with love and care! I paid less than 7 bucks for a tutu I and I feel proud of my self for researching to the point of finding gold!Recommend 2-6 years!My daughter is two ! Wears size 4t and this skirt ( one size ) fit perfect and will probaly be able to accommodate her quickly growing waist for some time!\n",
      "[8, 86, 1, 611, 8, 134, 9999, 6, 5996, 1122, 1, 84, 3, 567, 2, 9999, 2, 12, 13, 1811, 2, 255, 0, 6132, 6, 26, 440, 3268, 194, 19, 1510, 90, 19, 259, 9999, 1183, 479, 90, 17, 20, 55, 2, 623, 1, 611, 326, 52, 415, 1225, 8, 3, 6132, 1, 2, 1, 115, 3151, 7, 11, 1969, 8, 5697, 4, 0, 580, 7, 813, 9999, 3345, 9999, 414, 6, 134, 460, 29, 6679, 2, 10, 615, 37, 29, 32, 96, 2, 42, 9999, 28, 338, 4, 1768, 206, 473, 2884, 225, 8, 87, 76]\n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "print (X_sample[0])\n",
    "print (X_sample_ids[0])\n",
    "print (y_sample[0])\n",
    "\n",
    "print (X_test[0])\n",
    "print (X_test_ids[0])\n",
    "print (y_test[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Pad the sentence with differnt length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine the list of words index and label into a dataframe\n",
    "train_df = pd.DataFrame(np.column_stack([X_sample_ids,y_sample]), \n",
    "                               columns=['list_words', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 86, 1, 611, 8, 134, 10001, 6, 5996, 1122, ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[55, 10, 494, 270, 22, 46, 141, 661, 610, 1000...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          list_words  label\n",
       "0  [8, 86, 1, 611, 8, 134, 10001, 6, 5996, 1122, ... 1.0000\n",
       "1  [440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9... 1.0000\n",
       "2  [3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48... 0.0000\n",
       "3  [0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ... 1.0000\n",
       "4  [55, 10, 494, 270, 22, 46, 141, 661, 610, 1000... 0.0000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine the list of words index and label into a dataframe\n",
    "test_df = pd.DataFrame(np.column_stack([X_test_ids,y_test]), \n",
    "                               columns=['list_words', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>list_words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8, 86, 1, 611, 8, 134, 10001, 6, 5996, 1122, ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ...</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[55, 10, 494, 270, 22, 46, 141, 661, 610, 1000...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          list_words  label\n",
       "0  [8, 86, 1, 611, 8, 134, 10001, 6, 5996, 1122, ... 1.0000\n",
       "1  [440, 2, 46, 2232, 10, 6132, 6, 3, 255, 823, 9... 1.0000\n",
       "2  [3012, 1044, 2, 5, 6, 373, 25, 1038, 0, 80, 48... 0.0000\n",
       "3  [0, 1120, 1, 561, 10, 11, 1696, 9011, 3, 945, ... 1.0000\n",
       "4  [55, 10, 494, 270, 22, 46, 141, 661, 610, 1000... 0.0000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [26, 617, 400, 88, 2, 218, 4, 32, 7, 3405, 911...\n",
      "1    [60, 33, 1549, 4, 471, 2, 11, 153, 2, 412, 18,...\n",
      "2    [1, 18, 110, 103, 10, 281, 144, 223, 117, 158,...\n",
      "Name: list_words, dtype: object\n",
      "0   1.0000\n",
      "1   0.0000\n",
      "2   1.0000\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "##this is not necessary\n",
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor+n-1 > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "        return res['list_words'], res['label'] \n",
    "    \n",
    "data = SimpleDataIterator(train_df)\n",
    "a,b = data.next_batch(3)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##this helps to make sure each sentence have the same length (max_len)\n",
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n, max_len):\n",
    "        if self.cursor+n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "#             self.max_len = max_len\n",
    "        res = self.df.ix[self.cursor:self.cursor+n-1]\n",
    "        self.cursor += n\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        maxlen = max_len\n",
    "        x = np.zeros([n, maxlen], dtype=np.int32)\n",
    "        for i, x_i in enumerate(x):\n",
    "            l=len(res['list_words'].values[i]) ##list length\n",
    "            if l>maxlen: \n",
    "                x_i[:maxlen] = res['list_words'].values[i][:max_len]\n",
    "            else:\n",
    "                x_i[:l] = res['list_words'].values[i][:l]\n",
    "\n",
    "        return x, res['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pad = PaddedDataIterator(train_df)\n",
    "data_pad_test = PaddedDataIterator(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0   97  212 1500    3 1096    1   24   22  405   14    1   66    3\n",
      "   320   37    9    3  198   92    0 1386    6   26  187    0   97    6\n",
      "    46   90   36  126  787    5    3  243    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n",
      "0   1.0000\n",
      "Name: label, dtype: object\n",
      "[[  0  88   6  69  15   0 423 504 816  83  23 497  93   5  37  29  73  86\n",
      "   23  30   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "0   1.0000\n",
      "Name: label, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#sample call to get the data\n",
    "max_length = 100 \n",
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(1,max_length)\n",
    "batch_x_test, batch_y_test = PaddedDataIterator(test_df).next_batch(1,max_length)\n",
    "print (batch_x)\n",
    "print (batch_y)\n",
    "print (batch_x_test)\n",
    "print (batch_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.build RNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 50, 3, 100)\n",
      "(?, 50, 300)\n",
      "(?, 50, 3)\n",
      "50\n",
      "(?, 300)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PaddedDataIterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a1287b187fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m### GENERATE BATCH X,Y ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m##########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPaddedDataIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbatch_y_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PaddedDataIterator' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "\n",
    "\n",
    "'''\n",
    "To classify images using a recurrent neural network, we consider every image\n",
    "row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
    "handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "######################## \n",
    "### MODEL PARAMETERS ###\n",
    "######################## \n",
    "\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 1000\n",
    "display_step = 1\n",
    "sentence_size = 150\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 10  \n",
    "n_steps = sentence_size/n_input # timesteps\n",
    "n_hidden = 20 # hidden layer num of features\n",
    "n_classes = 2  \n",
    "vocab_size = 10000\n",
    "\n",
    "\n",
    "#################################\n",
    "### PLACEHOLDER AND VARIABLE ###\n",
    "#################################\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"int32\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"int32\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "######################## \n",
    "### DEFINE RNN MODEL ###\n",
    "######################## \n",
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for _ in xrange(num_layers):\n",
    "      cell = tf.contrib.rnn.BasicLSTMCell(H, forget_bias=0.0)\n",
    "      cell = tf.contrib.rnn.DropoutWrapper(\n",
    "          cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "      cells.append(cell)\n",
    "    return tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    with tf.name_scope(\"Embedding_Layer\"):\n",
    "        C_ = tf.Variable(tf.random_uniform([vocab_size, n_hidden], -1.0, 1.0), name=\"C\")\n",
    "        # embedding_lookup gives shape (batch_size, N, M)\n",
    "        test = tf.nn.embedding_lookup(C_, x)\n",
    "        print(test.shape)\n",
    "        x_ = tf.reshape(tf.nn.embedding_lookup(C_, x), \n",
    "                        [-1, n_steps, n_input*n_hidden], name=\"x\")\n",
    "        print(x_.shape)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    print(x.shape)\n",
    "    x = tf.unstack(x_, n_steps, 1)\n",
    "    print(len(x))\n",
    "    print(x[0].shape)\n",
    "    \n",
    "    with tf.variable_scope(\"first_lstm84\"):\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "#         lstm_cell = MakeFancyRNNCell(n_hidden, keep_prob = 1.0)\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "### LOSS AND OPTIMIZATION ###\n",
    "#############################\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    \n",
    "##################\n",
    "### EVALUATION ###\n",
    "################## \n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "#################\n",
    "### RUN GRAPH ###\n",
    "#################\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    iters=[]\n",
    "    costs=[]\n",
    "    accs=[]\n",
    "\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        \n",
    "        ##########################\n",
    "        ### GENERATE BATCH X,Y ###\n",
    "        ##########################\n",
    "        batch_x, batch_y_1 = PaddedDataIterator(train_df).next_batch(batch_size,sentence_size)\n",
    "        batch_y = np.concatenate((batch_y_1.reshape([-1,1]), 1-batch_y_1.reshape([-1,1])), axis=1)\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        \n",
    "        \n",
    "        ################\n",
    "        ### TRAINING ###\n",
    "        ################\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        \n",
    "        #################\n",
    "        ### REPORTING ###\n",
    "        #################\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy, loss\n",
    "            acc,loss = sess.run([accuracy,cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            iters.append(step*batch_size)\n",
    "            costs.append(loss)\n",
    "            accs.append(acc)\n",
    "#             print( sess.run(pred, feed_dict={x: batch_x, y: batch_y}))\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    ##########################\n",
    "    ### EVALUATE TEST DATA ###\n",
    "    ##########################    \n",
    "    \n",
    "    batch_x_test_1, batch_y_1_test = PaddedDataIterator(test_df).next_batch(len(batch_y),sentence_size)\n",
    "    batch_x_test = batch_x_test_1.reshape([-1,n_steps, n_input])\n",
    "    batch_y_test = np.concatenate((batch_y_1_test.reshape([-1,1]), 1-batch_y_1_test.reshape([-1,1])), axis=1)\n",
    "    test_acc, test_cost = sess.run([accuracy, cost], feed_dict={x: batch_x_test, y: batch_y_test})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAENCAYAAAD34uk0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGMuDiIiEsTyIiEgYy4OIiIT9f2QfPPUp3zrpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef99365d50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAENCAYAAAAVPvJNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef47505810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.319\n",
      "Testing cost: 0.74913\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple demo of a scatter plot.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#     accs=[]\n",
    "    \n",
    "x = iters\n",
    "y = costs\n",
    "z = accs\n",
    "\n",
    "plt.scatter(x, y )\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cost')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(x, z)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "print (\"Testing Accuracy:\",test_acc)\n",
    "print(\"Testing cost:\",test_cost)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,  176,   34,   33,   55,  473,    5,  371,   68,  111,    4,\n",
       "        2523,  133,    5,   34,  371,    3,  140,  539,    2,   55,    1,\n",
       "        2409,   22,    1,   64,  210,   20,  237,    5,  175,  335,   21,\n",
       "           5,  193,  271,    5, 1097,  326,   53,    1,  216,   57,    9,\n",
       "        1349,    1,   17,   10,  150,   15,  266,    4,  213,   31, 2480,\n",
       "           9,  849,    1,   17,   15, 2183, 1508,  426,   11,   15,  352,\n",
       "          13, 2397,  432,    4,    3,  179, 1624,   40,    0, 2087,  663,\n",
       "           4,  891,  819,    2,  215,  192,   11,  512,    0,  266,  822,\n",
       "           7,    3,   62,  207,   23,    1,   43,    2,  612,    5,  125,\n",
       "          23],\n",
       "       [   1,  182,   10,   91,    3,   62,  100,    3,  160,  329,  136,\n",
       "          12,    1,   78,  451,   13,    5,    1,  263,    2,  968,    5,\n",
       "           1,  124,    5,   16, 1273,    4, 1381,   36,   43,  131, 2129,\n",
       "        1394,    0, 1723,    4,   17,  101,  692,    0, 1882,   11,    0,\n",
       "        1361, 2415, 2062, 1912,   16,    0,  732,    5,  839,   12,    0,\n",
       "         695,    7,   20, 1188,   23,    6,  220,  968,    5,   21,    3,\n",
       "         110,  423,   99,  148,  451,   13, 2383, 2179,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 182,   10,  242,    8,   15,  194,    2,  313,  123,  601,  563,\n",
       "          11,    4,    5,  389,   14,    0,  538,    9,    0,  173,  563,\n",
       "         111,    0, 1992,  551,  882,  125,  638,    1,   72,    5,   41,\n",
       "          33,  251,   22,    6,   60,  124,  327,  421,    0,  242,   14,\n",
       "          22,    6,  112,    2,  124,  563,   11,   92,   40,   15,  194,\n",
       "          47,    6,   41,   17,    2, 2361,   16,   24,  165,  551,   26,\n",
       "         954,  429,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  10,   39, 1157, 1910,    4,  265, 2389,  125,    2,    3, 1613,\n",
       "         234, 1267,    7, 1455,    2,   35,    3,  179, 1495,   52,  169,\n",
       "        2577,   11, 2478, 2181, 1830,    4,   79,    3, 1563, 1538,   26,\n",
       "        1244,   27,  159,  440,  159,   14, 1975,   22,    6,  112,    3,\n",
       "          62,   50,  817,  130,   13,    0, 2066,  146, 1874,  196,   12,\n",
       "          63,  905,    0,  519,    7,    3,  440, 1450,   99,   51, 1502,\n",
       "         629,    9,    3, 1015,   39,   13,    3,  519,  196,    0,  449,\n",
       "           7,  553, 2402,  168,    0,  151, 1632,  114, 2212,    4, 2238,\n",
       "         198,    0,   65,   96,  410,    9,  121,    5,  212,  435,   14,\n",
       "          27],\n",
       "       [   1,  182,   10,  103,    2,  406,   96,  427,  171,  252,  370,\n",
       "          97,  461,   16,  439,  128,   34,  114,  472,    4,    0,  103,\n",
       "          18,    3,  493, 1120,   15,  427,  104,   20,   17,   31,  609,\n",
       "          23,   10, 1319,  103,  104,   27,  338,   68,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 478, 1983,   18,   42,    1, 2514,  365,    4,   45,  238, 2338,\n",
       "          78,  589,   13,    0, 2435,  155, 2322,  194,  566,    5,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [  74,  238,    1,  114, 1981,   42,    1,  328,    4,  492,    5,\n",
       "          11,  161,  784,    1, 1291,    4,   48,    2,   15,  767,    4,\n",
       "         533,   21,   68,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [ 110,   10,   16,  333,  546,  439, 1131,   52, 1845,   97,   24,\n",
       "         663,   26,  277,    8,  195,   26, 1115,    1,  745,  205,    5,\n",
       "         389,  209,  213,   24,  549,   43,   15,  195,  405,  860,    4,\n",
       "         426,   11,    5,  252,  225,   97,    0,  461,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0],\n",
       "       [   2,   15,   80,    9, 1787,   92,   19,   49,   96, 2589,  142,\n",
       "         215,    0,  579,   52, 1646,    3,  531, 2544,  404,  120,    7,\n",
       "           0,   39,  233,    5,    7,   20,   32,  391,    5,  249,   79,\n",
       "         500,   27,  111,   57,    9,   32,  190,  845,  642,    1,  165,\n",
       "         301,  211,    4,   79,    0, 2473,  500,  233,  422,  559,   37,\n",
       "           0, 2317,    9,    3,  815,   14,   10,   31,  534,    5,    2,\n",
       "           3,  381,  146,   77,  239,   33,  878,    2, 1862,   76,  380,\n",
       "           8, 1081,    5,  273, 1316,    7,    0,  465,   12,  598,    0,\n",
       "        1483,    9,  138,  503,  880,    2,  582, 1431,  812, 1533,  450,\n",
       "          10],\n",
       "       [  10,   18,    3,   32,  446,  426,   12,    1,  110,   11, 2123,\n",
       "         243,    7, 1373, 2559,   23,    5,  193,   20,  435,   97,    3,\n",
       "         549,   14,  268,    2,  157,    3, 1758,   26,    3, 1498,    8,\n",
       "         575, 2345,  591,  877,  574,  244, 2572,   30,  190,  313, 1213,\n",
       "        1776, 2340,   13, 1249,   30,  313, 1948, 1077,   26,  917,   22,\n",
       "           6, 1865,  207,  266,  103,   12,   30,   33, 1310,   59, 1795,\n",
       "           3,  681,  426,   11, 2225,  649,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]], dtype=int32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = PaddedDataIterator(train_df).next_batch(batch_size,100)\n",
    "batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "\n",
    "x = tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack_5:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:1' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:2' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:3' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:4' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:5' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:6' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:7' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:8' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'unstack_5:9' shape=(?, 10) dtype=float32>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(x, n_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##define variables \n",
    "n_hidden = 10 \n",
    "with tf.variable_scope(\"softmax\"):\n",
    "    W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "    b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "\n",
    "with tf.variable_scope(\"recurrent\"):\n",
    "    rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    initial_h_ = self.cell_.zero_state(self.batch_size_,dtype=tf.float32)\n",
    "    outputs, self.final_h_ = tf.nn.dynamic_rnn(self.cell_, inputs=self.x_,initial_state=self.initial_h_, sequence_length=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MakeFancyRNNCell(H, keep_prob, num_layers=1):\n",
    "    \"\"\"Make a fancy RNN cell.\n",
    "\n",
    "    Use tf.nn.rnn_cell functions to construct an LSTM cell.\n",
    "    Initialize forget_bias=0.0 for better training.\n",
    "\n",
    "    Args:\n",
    "      H: hidden state size\n",
    "      keep_prob: dropout keep prob (same for input and output)\n",
    "      num_layers: number of cell layers\n",
    "\n",
    "    Returns:\n",
    "      (tf.nn.rnn_cell.RNNCell) multi-layer LSTM cell with dropout\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for _ in xrange(num_layers):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(H, forget_bias=0.0)\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "        cell, input_keep_prob=keep_prob, output_keep_prob=keep_prob)\n",
    "        cells.append(cell)\n",
    "    return tf.contrib.rnn.MultiRNNCell(cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##have to fix here!!!\n",
    "def RNN(x):\n",
    "    n_hidden = 10 \n",
    "    with tf.variable_scope(\"softmax\"):\n",
    "        W_out_ = tf.Variable(tf.random_uniform([n_hidden,1], 0.0, 1.0), name=\"W_out_\")\n",
    "        b_out_ = tf.Variable(tf.zeros([1], dtype=tf.float32) ,name=\"b_out_\")\n",
    "    # reshape to [1, n_input]\n",
    "    n_input = len(x)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "#     print x\n",
    "    # 1-layer LSTM with n_hidden units.\n",
    "#     with tf.variable_scope('cell_def'):\n",
    "#         lstm_cell = MakeFancyRNNCell(H=n_hidden,keep_prob=1)\n",
    "    lstm_cell = tf.contrib.rnn.core_rnn_cell.BasicLSTMCell(n_hidden, forget_bias=0.0)\n",
    "#     print x\n",
    "    # generate prediction\n",
    "\n",
    "    \n",
    "#     with tf.variable_scope('rnn_def',reuse=True):\n",
    "    initial_h_ = lstm_cell.zero_state(1,dtype=tf.int32)\n",
    "    outputs,final_h_ = tf.contrib.rnn.static_rnn(lstm_cell, x,dtype=tf.int32)\n",
    "            \n",
    "            \n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], W_out_) + b_out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'static_rnn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-320bf1dbf5de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sample_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-c600588cd114>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     with tf.variable_scope('rnn_def',reuse=True):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_rnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'static_rnn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "\n",
    "learning_rate =0.1\n",
    "pred = RNN(X_sample_ids[0])\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y_sample[0]))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, loss, onehot_pred = session.run([optimizer, cost, pred], \n",
    "                                        feed_dict={x: symbols_in_keys, y: symbols_out_onehot})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60843,)\n",
      "(26177,)\n",
      "(60843, 1)\n",
      "(26177, 1)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print y_train.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sample = nonnan_doc_clean[0:100]\n",
    "y_sample = y[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "print len(X_sample[0].split())\n",
    "print X_sample[0]\n",
    "print y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.__getattr__('brown').words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = X_sample[0].split()\n",
    "bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['For' 'what' 'I' 'paid' 'for' 'two' 'tutus' 'is' 'unbeatable' 'anywhere!'\n",
      "  'I' 'ordered' 'a' 'pink' 'and' 'turquios' 'and' 'they' 'are' 'vibrant'\n",
      "  'and' 'beautiful!' 'The' 'tutu' 'is' 'very' 'full!' 'Princess' 'style!'\n",
      "  'Not' 'cheaply' 'made!' 'Not' 'cheap' 'materia!' 'Obviously' 'someone'\n",
      "  'made' 'these' 'with' 'love' 'and' 'care!' 'I' 'paid' 'less' 'than' '7'\n",
      "  'bucks' 'for' 'a' 'tutu' 'I' 'and' 'I' 'feel' 'proud' 'of' 'my' 'self'\n",
      "  'for' 'researching' 'to' 'the' 'point' 'of' 'finding' 'gold!Recommend'\n",
      "  '2-6' 'years!My' 'daughter' 'is' 'two' '!' 'Wears' 'size' '4t' 'and'\n",
      "  'this' 'skirt' '(' 'one' 'size' ')' 'fit' 'perfect' 'and' 'will'\n",
      "  'probaly' 'be' 'able' 'to' 'accommodate' 'her' 'quickly' 'growing'\n",
      "  'waist' 'for' 'some']]\n",
      "[ 0.875]\n"
     ]
    }
   ],
   "source": [
    "for i, (w,y) in enumerate(bi):\n",
    "    print w\n",
    "    print y_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from 'rnnlm.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "# reload(rnnlm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=False,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                 lm.target_y_:y,\n",
    "                 lm.learning_rate_:learning_rate,\n",
    "                 lm.initial_h_ : h}\n",
    "            \n",
    "        cost, _, h = session.run([loss, train_op,lm.final_h_],\n",
    "                       feed_dict=feed_dict)\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        #### END(YOUR CODE) ####\n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "\n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "\n",
    "    return total_cost / total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V=1000\n",
    "max_time = 20\n",
    "batch_size = 50\n",
    "learning_rate = 0.5\n",
    "num_epochs = 5\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=100, \n",
    "                    H=100, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=1)\n",
    "\n",
    "# TF_SAVEDIR = \"tf_saved\"\n",
    "# checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "# trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-576b79a3ba51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnnlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNNLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildCoreGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildTrainGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/266project/w266-final-project/rnnlm.py\u001b[0m in \u001b[0;36mBuildCoreGraph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_h_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_h_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/rnn.pyc\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((input_,))\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;31m# (B,T,D) => (T,B,D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     flat_input = tuple(array_ops.transpose(input_, [1, 0, 2])\n\u001b[0;32m--> 497\u001b[0;31m                        for input_ in flat_input)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m   \u001b[0mparallel_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_iterations\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, perm, name)\u001b[0m\n\u001b[1;32m   1268\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1270\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1271\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(x, perm, name)\u001b[0m\n\u001b[1;32m   3719\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3720\u001b[0m   \"\"\"\n\u001b[0;32m-> 3721\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transpose\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3722\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    766\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    767\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2336\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2338\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1717\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1720\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/legu/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension must be 2 but is 3 for 'recurrent/transpose' (op: 'Transpose') with input shapes: [?,100], [3]."
     ]
    }
   ],
   "source": [
    "# Will print status every this many seconds\n",
    "import rnnlm\n",
    "# import rnnlm_test\n",
    "reload(rnnlm)\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "# shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "# if not os.path.isdir(TF_SAVEDIR):\n",
    "#     os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "#         bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
    "        corpus = X_sample[0].split()\n",
    "        bi = utils.batch_generator(np.array(X_sample[0].split()), batch_size=1, max_time=100)\n",
    "        for i, (w,y) in enumerate(bi):\n",
    "            w1=w\n",
    "            \n",
    "        bi= (w1,y_sample[y])\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "        #### YOUR CODE HERE ####\n",
    "        # Run a training epoch.\n",
    "        cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=learning_rate, train=True, \n",
    "                     verbose=False, tick_s=3600)\n",
    "\n",
    "        #### END(YOUR CODE) ####\n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "    \n",
    "        ##\n",
    "        # score_dataset will run a forward pass over the entire dataset\n",
    "        # and report perplexity scores. This can be slow (around 1/2 to \n",
    "        # 1/4 as long as a full epoch), so you may want to comment it out\n",
    "        # to speed up training on a slow machine. Be sure to run it at the \n",
    "        # end to evaluate your score.\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        #score_dataset(lm, session, train_ids, name=\"Train set\")\n",
    "        print (\"[epoch %d]\" % epoch),\n",
    "        score_dataset(lm, session, test_ids, name=\"Test set\")\n",
    "        print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/legu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "corpus = nltk.corpus.brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vocabulary\n",
      "  Downloading Vocabulary-1.0.4.tar.gz\n",
      "Collecting requests==2.13.0 (from vocabulary)\n",
      "  Downloading requests-2.13.0-py2.py3-none-any.whl (584kB)\n",
      "\u001b[K    100% || 593kB 408kB/s \n",
      "\u001b[?25hCollecting mock==2.0.0 (from vocabulary)\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% || 61kB 1.9MB/s \n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): funcsigs>=1; python_version < \"3.3\" in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.9 in /Users/legu/anaconda/lib/python2.7/site-packages (from mock==2.0.0->vocabulary)\n",
      "Collecting pbr>=0.11 (from mock==2.0.0->vocabulary)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% || 102kB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: vocabulary\n",
      "  Running setup.py bdist_wheel for vocabulary ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/legu/Library/Caches/pip/wheels/36/6c/c0/92bb20f79402d055c3bce3e89d9f2cce5d6937bc2aadc0fb45\n",
      "Successfully built vocabulary\n",
      "Installing collected packages: requests, pbr, mock, vocabulary\n",
      "  Found existing installation: requests 2.11.1\n",
      "    Uninstalling requests-2.11.1:\n",
      "      Successfully uninstalled requests-2.11.1\n",
      "Successfully installed mock-2.0.0 pbr-3.1.1 requests-2.13.0 vocabulary-1.0.4\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'Vocabulary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-385fd0cf2b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Train set vocabulary: %d words\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'Vocabulary'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import vocabulary\n",
    "# train_sents, test_sents = utils.get_train_test_sents(corpus, split=0.8, shuffle=True)\n",
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in utils.flatten(corpus))\n",
    "print \"Train set vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
